<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ansiz</title>
  
  <subtitle>迎风向前是唯一的方法</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zxh.site/"/>
  <updated>2018-10-28T13:57:38.580Z</updated>
  <id>http://zxh.site/</id>
  
  <author>
    <name>张稀虹</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>HPC 系列文章(15):资源限制</title>
    <link href="http://zxh.site/2018/10/28/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-15-%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6/"/>
    <id>http://zxh.site/2018/10/28/HPC-系列文章-15-资源限制/</id>
    <published>2018-10-28T09:55:28.000Z</published>
    <updated>2018-10-28T13:57:38.580Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/assoc.png" alt="assoc"></p><p>前面讲到了Slurmdbd的配置，Slurmdbd负责存储作业记录，其实同时它还提供了账务信息的存储并负责资源限制的功能。默认情况下Slurm只会根据各集群中的基本配置限制作业，而不会启动跟账务系统相关联的资源限制，我们需要配置<code>AccountingStorageEnforce</code>选项以启用该功能。该配置项允许配置多个限制条件，以逗号分隔，他们分别是：</p><ul><li>associations - 关联账户系统，如果用户在数据库中不存在相关联的账户信息，将阻止用户运行作业。此选项将阻止用户访问无效帐户。</li><li>limits - 同时限制关联账户信息及QOS，设置该配置则默认会自动配置<code>associations</code></li><li>nojobs - 配置该选项后，账户系统不会记录作业信息，设置该配置则默认会自动配置<code>nosteps</code></li><li>nosteps - 配置该选项后账务系统将不会记录作业步信息</li><li>qos - 配置该选项后系统会要求每个作业必须指定有效的QOS后才允许运行</li><li>safe - 该配置要求作业必须有关联行账户信息或者有效的QOS，并且必须配置GrpTRESMins</li><li>wckeys - 这将阻止用户在他们无权访问的wckey下运行作业，关于wckey可以查阅slurm文档，这里暂时不做详细介绍。</li></ul><div class="note info"><p>nojobs和nosteps听起来很奇怪，配置后将不会记录作业信息，这不是相当于阉割掉了一部分slurmdbd的功能吗？实际上这在你想要使用资源限制但不关心利用率的环境中可能会有所帮助。 </p></div><a id="more"></a><h2 id="资源限制"><a href="#资源限制" class="headerlink" title="资源限制"></a>资源限制</h2><p>上面介绍了如何启用Slurm的Accounting资源限制，可以看到它提供了多种不同的限制条件，你可以设置一个或多个限制条件。Slurm的资源限制是一个多因素叠加的限制，包括QOS、Association以及Partition limit等等。它们之间遵循以下的优先级规则：</p><ol><li>Partition QOS limit</li><li>Job QOS limit</li><li>User association</li><li>Account association(s), ascending the hierarchy</li><li>Root/Cluster association</li><li>Partition limit</li><li>None</li></ol><p>当限制条件出现在多个不同的层级时，以首次出现的限制条件为准。例如有以下限制规则：</p><ul><li>partition QOS中MaxJobs=20。</li><li>job QOS中没有任何限制。</li><li>User association中MaxJobs=4， MaxSubmitJobs=50</li></ul><p>叠加到一起后最终的限制结果是MaxJobs=20，MaxSubmitJobs=50。</p><h2 id="Association"><a href="#Association" class="headerlink" title="Association"></a>Association</h2><p>前面总是提到QOS和Association两个词，究竟是什么东西？我们先说Association。根据官方文档的解释，Association是一个4元组，由群集名称，帐户，用户和（可选的）Slurm分区组成。简单的说它就是一个由集群名、账务系统的账户名、用户名和Slurm分区所组成的一个唯一的关联关系，你可以为这个关系设定一些资源限制，当作业提交时则会校验相应的限制。</p><p>你可以通过sacctmgr命令进行association的管理，下面是一些基本操作，更多可以参考<a href="https://slurm.schedmd.com/sacctmgr.html" target="_blank" rel="noopener">sacctmgr文档</a>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 创建一个名为cluster1的集群</span></span><br><span class="line">sacctmgr create cluster cluster1</span><br><span class="line"><span class="comment">## 创建一个test账户</span></span><br><span class="line">sacctmgt create account name=<span class="built_in">test</span></span><br><span class="line"><span class="comment">## 添加一个用户到test账户</span></span><br><span class="line">sacctmgr add user xhzhang account=<span class="built_in">test</span></span><br><span class="line"><span class="comment">## 删除一个用户</span></span><br><span class="line">sacctmgr delete user name=xhzhang cluster=cluster1 account=<span class="built_in">test</span></span><br><span class="line"><span class="comment">## 列出所有的assocation</span></span><br><span class="line">sacctmgr show assoc</span><br></pre></td></tr></table></figure><h2 id="QOS"><a href="#QOS" class="headerlink" title="QOS"></a>QOS</h2><p>QOS（Quality of Service）是一个我们常听说的词，它是一种控制机制，可以针对性的根据用户优先级或者不同的规则提供不同服务供给，以保障资源的更合理利用。在Slurm中也有QOS，它主要提供以下几个功能：</p><ul><li>作业调度优先级调控</li><li>作业抢占</li><li>作业资源限制</li><li>队列资源限制</li></ul><p>QOS也是通过sacctmgr工具进行管理，这里列举一些基本操作，更多可以参考<a href="https://slurm.schedmd.com/qos.html" target="_blank" rel="noopener">QOS文档</a>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 查看所有QOS配置</span></span><br><span class="line">sacctmgr show qos</span><br><span class="line"><span class="comment">## 添加新的QOS规则</span></span><br><span class="line">sacctmgr add qos <span class="built_in">test</span></span><br><span class="line"><span class="comment">## 修改QOS优先级</span></span><br><span class="line">sacctmgr modify qos <span class="built_in">test</span> <span class="built_in">set</span> priority=10</span><br><span class="line"><span class="comment">## 配置QOS资源限制</span></span><br><span class="line">sacctmgr modify qos <span class="built_in">test</span> <span class="built_in">set</span> GrpCPUs=10</span><br><span class="line"><span class="comment">## 为用户分配QOS限制</span></span><br><span class="line">sacctmgr modify user xhzhang <span class="built_in">set</span> qos=<span class="built_in">test</span></span><br><span class="line"><span class="comment">## 单个用户允许添加多个QOS</span></span><br><span class="line">sacctmgr modify user xhzhang <span class="built_in">set</span> qos+=qos2</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>QOS和Association的区别是QOS是一个一对多的限制条件，你可以为多个association配置同一个QOS，而Association则是一个特定的规则，它具备一定的继承性，但是最终生效的只有唯一的一个association，最终的限制都是对association上规则的校验。</p><p>QOS和Association都能配置很多限制规则，例如：</p><ul><li>GrpTRESMins： 限制TRES资源的总可用分钟数。</li><li>GrpJobs： 限制允许同时运行的最大作业数量。</li><li>MaxTRESMinsPerJob： 每个作业可用多最大TRES资源分钟数。</li><li>GrpSubmitJobs: 允许提交的最大作业数量。</li><li>…</li></ul><p>上面列出的只是一部分限制条件，Slurm还提供了更多维度的多种限制条件，更多选项可以参考<a href="https://slurm.schedmd.com/resource_limits.html" target="_blank" rel="noopener">资源限制文档</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/assoc.png&quot; alt=&quot;assoc&quot;&gt;&lt;/p&gt;
&lt;p&gt;前面讲到了Slurmdbd的配置，Slurmdbd负责存储作业记录，其实同时它还提供了账务信息的存储并负责资源限制的功能。默认情况下Slurm只会根据各集群中的基本配置限制作业，而不会启动跟账务系统相关联的资源限制，我们需要配置&lt;code&gt;AccountingStorageEnforce&lt;/code&gt;选项以启用该功能。该配置项允许配置多个限制条件，以逗号分隔，他们分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;associations - 关联账户系统，如果用户在数据库中不存在相关联的账户信息，将阻止用户运行作业。此选项将阻止用户访问无效帐户。&lt;/li&gt;
&lt;li&gt;limits - 同时限制关联账户信息及QOS，设置该配置则默认会自动配置&lt;code&gt;associations&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;nojobs - 配置该选项后，账户系统不会记录作业信息，设置该配置则默认会自动配置&lt;code&gt;nosteps&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;nosteps - 配置该选项后账务系统将不会记录作业步信息&lt;/li&gt;
&lt;li&gt;qos - 配置该选项后系统会要求每个作业必须指定有效的QOS后才允许运行&lt;/li&gt;
&lt;li&gt;safe - 该配置要求作业必须有关联行账户信息或者有效的QOS，并且必须配置GrpTRESMins&lt;/li&gt;
&lt;li&gt;wckeys - 这将阻止用户在他们无权访问的wckey下运行作业，关于wckey可以查阅slurm文档，这里暂时不做详细介绍。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;nojobs和nosteps听起来很奇怪，配置后将不会记录作业信息，这不是相当于阉割掉了一部分slurmdbd的功能吗？实际上这在你想要使用资源限制但不关心利用率的环境中可能会有所帮助。 &lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC 系列文章(14):账务系统（Account）</title>
    <link href="http://zxh.site/2018/10/21/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-14-%E8%B4%A6%E5%8A%A1%E7%B3%BB%E7%BB%9F%EF%BC%88Account%EF%BC%89/"/>
    <id>http://zxh.site/2018/10/21/HPC-系列文章-14-账务系统（Account）/</id>
    <published>2018-10-21T09:06:57.000Z</published>
    <updated>2018-10-28T11:13:43.509Z</updated>
    
    <content type="html"><![CDATA[<p>经过前面的内容，大家应该对于Slurm最基本的功能使用有了认识。不过对于在生产环境中集群多用户使用作业调度系统，其实还没有深入了解。例如集群环境下如何统一作业运行时的用户权限？Slurm的资源限制究竟是如何实现的？如何为不同的用户配置不同的资源限制策略？如何得到集群资源使用情况的数据报表等等。下面我们就介绍这部分内容，先从Slurm的账务系统（accounting）说起。<a id="more"></a></p><h2 id="账务统计"><a href="#账务统计" class="headerlink" title="账务统计"></a>账务统计</h2><p>当我们将集群资源开放给多个用户使用时，肯定希望能为不同的用户设定不同的资源使用策略，并且希望知道整个集群的使用率以及作业的各种历史数据等等，也就是说我们希望通过查询历史记录来统计分析这些信息，既然要查询历史记录，那我们就应该有地方做数据存储，这就是Slurm的Accounting系统负责的工作。</p><h3 id="状态临时存储"><a href="#状态临时存储" class="headerlink" title="状态临时存储"></a>状态临时存储</h3><p>在介绍Accounting之前，我们先回顾一下作业查询并说一下它们的区别。之前介绍过用<code>squeue</code>和<code>scontrol show job</code>的方式查询作业，不过你可能会发现这些命令所查询到的作业，并不是历史作业，而是正在运行和刚运行完不久的作业。这似乎也很好理解，毕竟如果通过这样的方式查询到的是所有的历史作业的话，一个包含大量作业的多用户系统光是查询作业就可能把服务拖垮了。不过更为关键的是，这样的查询方式实际上并不是从Accounting系统中所查询出来的，而是保存在slurm的临时文件中的。打开slurm.conf配置文件，其中有一个<code>StateSaveLocation</code>的配置项（默认值是/tmp)，它指定了Slurmctld工作时存储状态的临时文件路径。</p><p><img src="http://img.chinahpc.club/2018/10/slurm-tmp-file.png" alt="tmp-files"></p><p>去到这个目录下，你会看到很多由slurm所创建的文件，这些文件对于Slurm而言至关重要，包括job_state,node_state等等，他们存储了当前的各种状态。Slurmctld会不断更新这些文件，当机器意外重启后，也会根据这些文件来恢复工作。我们通过<code>squeue</code>和<code>scontrol show job</code>查询到的作业就是对加载到内存中的这个文件的解析。因为需要被加载到内存中，Slurm通过以下两种方式防止内存被耗尽：</p><ol><li>Slurm不允许无限制的提交作业，MaxJobCount限制了系统中允许运行的最大作业量。</li><li>Slurm并不会无限期对存储作业状态，而是会定期清理掉已经结束运行的作业（包括错误退出以及正常退出的作业），通过MinJobAge限制作业完成后最大存留时长（默认为5分钟后清理）</li></ol><p>了解这些内容有助于你更好的了解Slurm的工作原理，不过这并不是我们所希望的持久化的数据记录。</p><h4 id="扩展（如何快速删除大量的作业）"><a href="#扩展（如何快速删除大量的作业）" class="headerlink" title="扩展（如何快速删除大量的作业）"></a>扩展（如何快速删除大量的作业）</h4><p>因为上述状态文件记录了各种数据，所以通常情况下是不允许删除和修改的，随意修改和删除可能会导致数据丢失。不过如果你不小心写错了脚本往系统中提交了数以万计的作业，想快速删除掉这些作业，就可以利用删除文件的方式快速清除掉这些作业。</p><p>Slurm本身提供了scancel -u $uid的方式批量删除某个用户提交的所有作业，但是这对于超大规模的作业来说并不好使。Slurm的作业取消中有一个比较特殊的逻辑，我们可以看一段儿源码：</p><p><img src="http://img.chinahpc.club/2018/10/source1.jpeg" alt="source"></p><p>简单阅读代码可知，为了防止RPC DOS攻击，当系统中大量取消作业时，会自动以斐波那契规律递增一个延迟，直到这个时间变成一秒。简单的说就是如果你删除大量的作业，系统会自动将请求速度降低到1秒钟一个，如果你不小心通过特殊的脚本提交了太多的作业，这个删除效率依然是比较低的。此时更加方便的方式是将slurmctld服务停止，然后删除job_state文件再重启服务。这样就可以快速的删掉所有的作业了。</p><h3 id="持久化存储"><a href="#持久化存储" class="headerlink" title="持久化存储"></a>持久化存储</h3><h4 id="文件存储"><a href="#文件存储" class="headerlink" title="文件存储"></a>文件存储</h4><p>持久化存储才是我们要讨论的主题，Slurm可以将作业信息收集存储到文件或者数据库中。最简单的是存放到文件中，你只需要配置<code>AccountingStorageType=accounting_storage/filetxt</code>或者 <code>JobCompType=jobcomp/filetxt</code>并指定存储路径<code>AccountingStorageLoc</code>或者<code>JobCompLoc</code>，不过这并不是最理想的方式，你需要自行rotate文件，并且它使得Slurm作业无法与账户信息相关联。</p><h4 id="数据库存储"><a href="#数据库存储" class="headerlink" title="数据库存储"></a>数据库存储</h4><p>利用slurmdbd将数据存储到数据库则是更有吸引力的方案。</p><h5 id="数据库部署"><a href="#数据库部署" class="headerlink" title="数据库部署"></a>数据库部署</h5><p>首先我们需要安装数据库（Mysql或者Mariadb），这一步很简单就不赘述了。不过你需要适当的做一些优化，尤其是对于作业规模稍大的集群。主要有以下几点：</p><ul><li>使用InnoDB引擎</li><li>适当增大innodb_buffer_pool_size</li><li>建议将innodb_lock_wait_timeout和innodb_log_file_size设置为大于默认值的值</li></ul><p>示例配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; SHOW VARIABLES LIKE &apos;innodb_buffer_pool_size&apos;;</span><br><span class="line">+-------------------------+-----------+</span><br><span class="line">| Variable_name           | Value     |</span><br><span class="line">+-------------------------+-----------+</span><br><span class="line">| innodb_buffer_pool_size | 134217728 |</span><br><span class="line">+-------------------------+-----------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">$cat my.cnf</span><br><span class="line">...</span><br><span class="line">[mysqld]</span><br><span class="line">innodb_buffer_pool_size=1024M</span><br><span class="line">innodb_log_file_size=64M</span><br><span class="line">innodb_lock_wait_timeout=900</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h5 id="slurmdbd服务部署"><a href="#slurmdbd服务部署" class="headerlink" title="slurmdbd服务部署"></a>slurmdbd服务部署</h5><p>slurmdbd是一个独立的服务，你需要单独安装它，它也可以独立部署在一台不包含其他slurm守护进程的服务器上，数据库也可以部署到任意服务器。直接使用rpm安装即可，然后你需要修改一下slurm配置以启用slurmdbd进行统计（为了方便，这里我给出的是比较基本的配置并且给出建议值）：</p><ul><li>AccountingStorageType: 设置为”accounting_storage/slurmdbd”.</li><li>AccountingStorageEnforce：限制条件，以逗号分隔，可用的值包括：associations,limits,nojobs,nosteps,qos,safe,wckeys等，具体内容我们后面再详细讲解，可以参考一下<a href="https://slurm.schedmd.com/accounting.html" target="_blank" rel="noopener">slurm文档</a>。</li><li>AccountingStorageHost: Slurmdbd服务地址，如果部署在本地可以直接写localhost</li><li>AccountingStoragePort: Slurmdbd服务端口号，默认6819</li><li>ClusterName：集群名称，因为支持多集群，所以要为每个集群配置一个名称</li></ul><p>然后需要配置一下slurmdbd.conf，这里我给出一个基本配置文件，具体项的意思可以直接查<a href="https://slurm.schedmd.com/accounting.html#slurmdbd-configuration" target="_blank" rel="noopener">文档</a>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#</span><br><span class="line"># Example slurmdbd.conf file.</span><br><span class="line">#</span><br><span class="line"># See the slurmdbd.conf man page for more information.</span><br><span class="line">#</span><br><span class="line"># Archive info</span><br><span class="line">#ArchiveJobs=yes</span><br><span class="line">#ArchiveDir=&quot;/tmp&quot;</span><br><span class="line">#ArchiveSteps=yes</span><br><span class="line">#ArchiveScript=</span><br><span class="line">#JobPurge=12</span><br><span class="line">#StepPurge=1</span><br><span class="line">#</span><br><span class="line"># Authentication info</span><br><span class="line">AuthType=auth/munge</span><br><span class="line">#AuthInfo=/var/run/munge/munge.socket.2</span><br><span class="line">#</span><br><span class="line"># slurmDBD info</span><br><span class="line">DbdAddr=localhost</span><br><span class="line">DbdHost=localhost</span><br><span class="line">#DbdPort=7031</span><br><span class="line">SlurmUser=slurm</span><br><span class="line">#MessageTimeout=300</span><br><span class="line">DebugLevel=4</span><br><span class="line">#DefaultQOS=normal,standby</span><br><span class="line">LogFile=/var/log/slurm/slurmdbd.log</span><br><span class="line">PidFile=/var/run/slurmdbd.pid</span><br><span class="line">#PluginDir=/usr/lib/slurm</span><br><span class="line">#PrivateData=accounts,users,usage,jobs</span><br><span class="line">#TrackWCKey=yes</span><br><span class="line">#</span><br><span class="line"># Database info</span><br><span class="line">StorageType=accounting_storage/mysql</span><br><span class="line">StorageHost=localhost</span><br><span class="line">#StoragePort=1234</span><br><span class="line">#StoragePass=some_pass</span><br><span class="line">StoragePass=slurm</span><br><span class="line">StorageUser=slurm</span><br><span class="line">StorageLoc=slurm_acct_db</span><br></pre></td></tr></table></figure><p>启动mysql、slurmdbd，并重启slurmctld，各服务正常则说明配置完成了。后面我们再介绍具体的概念和使用方式。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;经过前面的内容，大家应该对于Slurm最基本的功能使用有了认识。不过对于在生产环境中集群多用户使用作业调度系统，其实还没有深入了解。例如集群环境下如何统一作业运行时的用户权限？Slurm的资源限制究竟是如何实现的？如何为不同的用户配置不同的资源限制策略？如何得到集群资源使用情况的数据报表等等。下面我们就介绍这部分内容，先从Slurm的账务系统（accounting）说起。
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC 系列文章(13):GPU调度</title>
    <link href="http://zxh.site/2018/10/14/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-13-GPU%E8%B0%83%E5%BA%A6/"/>
    <id>http://zxh.site/2018/10/14/HPC-系列文章-13-GPU调度/</id>
    <published>2018-10-14T13:47:13.000Z</published>
    <updated>2018-10-21T11:11:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/2018/10/gres-gpu.png" alt="gres-gpu"></p><p>前面介绍过了如何为作业指定资源参数，不过我们只涉及到了节点数量、CPU和内存相关的指定。而专业的集群调度系统，应该支持多种复杂的硬件资源管理调度策略，为集群内各种硬件提供统一的资源调度管理。例如目前很火热的GPU集群，就需要统一管理调度GPU资源。通用资源包括但不限于GPU、MIC、NIC等设备的管理，Slurm通过一种灵活的插件机制实现了“通用资源”的管理，你甚至可以自己开发专用的通用资源管理插件以支持特殊的设备管理。GPU是最为常见的，所以我们这里以GPU为例进行介绍。<a id="more"></a></p><h2 id="通用资源管理配置"><a href="#通用资源管理配置" class="headerlink" title="通用资源管理配置"></a>通用资源管理配置</h2><p>Slurm中通用资源被称之为GRES(Generic Resource)，默认情况下Slurm是没有开启该功能的，我们需要修改一下slurm.conf配置文件，在GresTypes中指定要管理的资源类型，例如gpu。</p><p>然后你需要为节点配置具体的通用资源参数，参数格式：<code>&lt;name&gt;[:&lt;type&gt;][:no_consume]:&lt;number&gt;[K|M|G]</code>：</p><ul><li>name: 即通用资源的名称，与GresTypes相对应</li><li>type：可选的参数，你可以通过设置Type来标记不同的设备型号</li><li>no_consume：设置为no_consume则作业不会独占该设备</li><li>number[K|M|G]：数量及其单位，例如1，或者4G</li></ul><p>多个设备资源应以逗号分隔开，示例配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Configure support for our four GPUs</span><br><span class="line">GresTypes=gpu,bandwidth</span><br><span class="line">NodeName=node02 Gres=gpu:tesla:2,gpu:kepler:2,bandwidth:lustre:no_consume:4G</span><br></pre></td></tr></table></figure><h2 id="通用资源配置"><a href="#通用资源配置" class="headerlink" title="通用资源配置"></a>通用资源配置</h2><p>上述文件让<code>Slurmctld</code>知道了系统中有哪些通用资源可以调配，不过<code>Slurmd</code>却还不知道具体如何使用它们，所以配置完上述信息后，我们还要为每一个具有通用资源设备的计算节点编写配置<code>gres.conf</code>文件，它通常和slurm.conf位于同一文件夹中，如果没有你需要手动创建该文件。该文件描述了节点上所存在的通用资源设备具体的数量、关联的设备文件以及可使用的核心等信息，具体包含了：</p><ul><li>Name：即通用资源的名称，与GresTypes相对应</li><li>Count：资源量，默认情况下等于指定的设备文件的数量，但对于某些特殊的资源，例如共享存储空间等，你可以为其添加单位[K/M/G]</li><li>CPUs：允许使用的核心，不指定则默认所有核心均可用</li><li>File：与资源关联的设备文件的完整路径名，这对于资源限制来说至关重要，因为Slurm中需要使用cgroups进行资源限制，而cgroups则需要配置具体的设备文件。（关于slurm的资源限制原理我们后面再展开讲）</li><li>Type：可选的参数，你可以通过设置Type来标记不同的设备型号。</li></ul><p>示例配置文件（包含了两个不同的型号的4张GPU卡）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Configure support for our four GPUs, plus bandwidth</span><br><span class="line">Name=gpu Type=tesla  File=/dev/nvidia0 CPUs=0,1</span><br><span class="line">Name=gpu Type=tesla  File=/dev/nvidia1 CPUs=0,1</span><br><span class="line">Name=gpu Type=kepler File=/dev/nvidia2 CPUs=2,3</span><br><span class="line">Name=gpu Type=kepler File=/dev/nvidia3 CPUs=2,3</span><br></pre></td></tr></table></figure><h2 id="提交作业"><a href="#提交作业" class="headerlink" title="提交作业"></a>提交作业</h2><p>配置完成之后，我们需要重启所有的slurmd及slurmctld服务，然后就可以通过作业提交时的<code>--gres</code>参数申请具体的通用资源了，我编写了一个脚本来验证通用资源的使用，内容如下(<code>nvidia-smi</code>是NVIDIA系列显卡提供的命令行显卡管理工具）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">date</span><br><span class="line">nvidia-smi</span><br><span class="line">hostname</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"done"</span></span><br></pre></td></tr></table></figure><p>用如下命令提交：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">srun -p calulate -w node02 --gres=gpu:2 ./nv.sh</span><br></pre></td></tr></table></figure><p>输出结果：</p><p><img src="http://img.chinahpc.club/2018/10/gres-gpu.png" alt="gres-gpu"></p><p>该输出结果表明这个运行在node02上的作业得到了2张GPU卡的资源。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/2018/10/gres-gpu.png&quot; alt=&quot;gres-gpu&quot;&gt;&lt;/p&gt;
&lt;p&gt;前面介绍过了如何为作业指定资源参数，不过我们只涉及到了节点数量、CPU和内存相关的指定。而专业的集群调度系统，应该支持多种复杂的硬件资源管理调度策略，为集群内各种硬件提供统一的资源调度管理。例如目前很火热的GPU集群，就需要统一管理调度GPU资源。通用资源包括但不限于GPU、MIC、NIC等设备的管理，Slurm通过一种灵活的插件机制实现了“通用资源”的管理，你甚至可以自己开发专用的通用资源管理插件以支持特殊的设备管理。GPU是最为常见的，所以我们这里以GPU为例进行介绍。
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC 系列文章(12):交互作业</title>
    <link href="http://zxh.site/2018/10/07/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-12-%E4%BA%A4%E4%BA%92%E4%BD%9C%E4%B8%9A/"/>
    <id>http://zxh.site/2018/10/07/HPC-系列文章-12-交互作业/</id>
    <published>2018-10-07T12:12:15.000Z</published>
    <updated>2018-10-21T11:13:24.954Z</updated>
    
    <content type="html"><![CDATA[<p>前面介绍了如何提交作业，不过我们只讲了最简单的作业提交。实际上Slurm的作业提交方式也有多种，将所需的作业编写成脚本再用<code>sbatch</code>命令提交是最为常见的方式，不过这种方式存在弊 端，你可能会因为脚本中某一个命令执行失败而反复编辑脚本，再不断的查看新提交的任务的输出，在调试时显得很不灵活；又或者你的程序需要交互式的应答才能跑通，这时候<code>srun</code>和<code>salloc</code>则是更好的方式。</p><h2 id="salloc"><a href="#salloc" class="headerlink" title="salloc"></a>salloc</h2><p>salloc从字面上就能看出来，实际上是”slurm allocate”的缩写，在解释salloc之前，我们先回顾一下之前对于作业和作业步的定义：</p><blockquote><ul><li>作业（job）：<strong>特定时间为用户进行的一次资源申请或分配即可看作一个作业</strong>。这和我们惯性思维中的作业概念并不一致，传统意义上我们总是认为作业应该是某个运行的脚本或者程序，但事实上Slurm的作业只代表一次资源申请或分配。理解这个区别将有利于你理解Slurm中一些比较高级的用法。</li><li>作业步（job step）：Slurm中有作业步的概念，你可以理解为子作业。这允许我们在某个作业中分步骤的细分使用计算资源。<a id="more"></a></li></ul></blockquote><p>无论是sbatch、srun还是salloc，<strong>其本质都是一种对资源的申请并将你所要执行的内容运行在限定的资源内的一种行为</strong>，三者提交时对资源指定的参数都是一样的，例如：</p><pre><code>salloc -N 2 -n 2 -p node-all</code></pre><p><img src="http://img.chinahpc.club/2018/10/salloc.png" alt="salloc"></p><p>如果当前资源满足的话，将会显示该资源已分配，你似乎没察觉到任何变化，但实际上你当前使用的shell程序（泛指sh、bash、zsh等）已经发生了变化。你进入了一个新的环境下，不信你可以试试执行exit，你会得到如下输出：</p><pre><code>salloc: Relinquishing job allocation 5914</code></pre><p>这正是退出salloc任务的方式。</p><p>那么这个新的环境有什么玄机呢？实际上slurm在这个过程中完成了资源的申请，然后为新的bash进程配置了一堆环境变量，我们可以通过env来查看，会得到类似如下输出：</p><p><img src="http://img.chinahpc.club/2018/10/env.png" alt="env"></p><p>可以看到其包含了本次作业的各项参数，包括申请到的资源等信息。你肯定好奇那如果我手动修改环境变量，是不是可以改变本次作业的资源呢？答案是当然不能。这些环境变量是可读的，但仅仅是为了让你的作业可以获知这些信息，作业真正的资源分配信息依然是保存在特定的文件中的（这个后面再介绍）</p><p>在这个环境下你可以通过<code>srun</code>命令提交任意命令，根据我的设置，每条命令会执行两次，分别在两个节点上。我们可以通过执行命令验证一下：</p><p><img src="http://img.chinahpc.club/2018/10/salloc-srun.png" alt="srun"></p><p>这就是salloc的使用方式。</p><h2 id="srun"><a href="#srun" class="headerlink" title="srun"></a>srun</h2><p>上面的试验中，我们使用了srun提交命令，看到一次输出了两个hostname，证明该命令同时在两个节点上被执行了。事实上在作业中通过srun执行命令才是是slurm中正确使用多个节点资源的方式。srun不仅仅可以在salloc中使用，你可以在sbatch的脚本中使用srun，还可以直接使用srun，在某个作业中多次调用srun指令会生成多个作业步。</p><p>你还可以单独使用srun，这样会生成一个单步的作业，执行完成后就会退出。</p><p>如果我们执行下面的命令，会发生什么呢？</p><pre><code>srun bash</code></pre><p>slurm会为你分配一个节点并运行bash，此时你执行的任何操作，都相当于在该分配的节点上运行，如果我们加上–pty参数，就会更明了了。</p><pre><code>srun --pty bash</code></pre><p>这样你会得到类似salloc的体验，不过当前运行的bash是运行在你分配到的节点上的。</p><p>上面的内容，一定要自己手动操作一遍才会彻底理解。后面我们会介绍一下这样的使用方式具体能实现什么功能。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面介绍了如何提交作业，不过我们只讲了最简单的作业提交。实际上Slurm的作业提交方式也有多种，将所需的作业编写成脚本再用&lt;code&gt;sbatch&lt;/code&gt;命令提交是最为常见的方式，不过这种方式存在弊 端，你可能会因为脚本中某一个命令执行失败而反复编辑脚本，再不断的查看新提交的任务的输出，在调试时显得很不灵活；又或者你的程序需要交互式的应答才能跑通，这时候&lt;code&gt;srun&lt;/code&gt;和&lt;code&gt;salloc&lt;/code&gt;则是更好的方式。&lt;/p&gt;
&lt;h2 id=&quot;salloc&quot;&gt;&lt;a href=&quot;#salloc&quot; class=&quot;headerlink&quot; title=&quot;salloc&quot;&gt;&lt;/a&gt;salloc&lt;/h2&gt;&lt;p&gt;salloc从字面上就能看出来，实际上是”slurm allocate”的缩写，在解释salloc之前，我们先回顾一下之前对于作业和作业步的定义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;作业（job）：&lt;strong&gt;特定时间为用户进行的一次资源申请或分配即可看作一个作业&lt;/strong&gt;。这和我们惯性思维中的作业概念并不一致，传统意义上我们总是认为作业应该是某个运行的脚本或者程序，但事实上Slurm的作业只代表一次资源申请或分配。理解这个区别将有利于你理解Slurm中一些比较高级的用法。&lt;/li&gt;
&lt;li&gt;作业步（job step）：Slurm中有作业步的概念，你可以理解为子作业。这允许我们在某个作业中分步骤的细分使用计算资源。
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC 系列文章(11):节点状态</title>
    <link href="http://zxh.site/2018/09/28/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-11-%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81/"/>
    <id>http://zxh.site/2018/09/28/HPC-系列文章-11-节点状态/</id>
    <published>2018-09-28T07:33:34.000Z</published>
    <updated>2018-10-21T10:57:46.070Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/sinfo.png" alt="sinfo"></p><p>前面介绍了如何查看作业的状态，下面我们介绍一下如何查看节点的状态。跟作业状态查看类似，节点状态也可以通过命令行获取，有两种方式：<code>sinfo</code>和<code>scontrol show node</code>，命令行工具的使用时非常简单的，但是节点的状态理解比较复杂。还是先从简单的介绍命令行工具开始吧。<a id="more"></a></p><h2 id="命令行工具"><a href="#命令行工具" class="headerlink" title="命令行工具"></a>命令行工具</h2><h3 id="sinfo"><a href="#sinfo" class="headerlink" title="sinfo"></a>sinfo</h3><p>查看节点信息最简单的方式是直接使用sinfo命令，该命令会输出当前系统中的所有队列及节点的基本状态，如果只是快速查看一下可用的节点等信息，sinfo所提供的信息就足够了。</p><p><img src="http://img.chinahpc.club/sinfo.png" alt="sinfo"></p><p>此外sinfo还提供了一些简单的参数，用于提供格式化或者过滤特定节点等功能，都比较简单，可以通过–help自行查看，这里就不赘述了。</p><p><img src="http://img.chinahpc.club/2018/09/sinfo-help.png" alt="sinfo-help"></p><h3 id="scontrol"><a href="#scontrol" class="headerlink" title="scontrol"></a>scontrol</h3><p>如果想查看更为详细的节点状态信息，则应该使用命令</p><pre><code>scontrol show node [nodename]</code></pre><p>该命令会输出节点的详细信息，包括该节点的各种资源使用状态及负载，还包括该节点的系统信息、软件信息及启动信息等等，如果想要通过slurm了解具体节点的详情，则应用使用该方式。</p><p><img src="http://img.chinahpc.club/2018/09/show-node.png" alt="show-node"></p><h2 id="Slurm节点状态"><a href="#Slurm节点状态" class="headerlink" title="Slurm节点状态"></a>Slurm节点状态</h2><p>Slurm中节点的状态非常多，而且节点状态由状态和特殊符号两部分构成。节点状态中可能会有一个特殊的字符后缀用于标识节点关联的状态标志。组合起来的作业状态就更复杂了，经验不足的使用者想要理解还是有一些难度的。这里我以官方文件加上我自己的理解逐一解释各个状态。</p><h3 id="特殊字符"><a href="#特殊字符" class="headerlink" title="特殊字符"></a>特殊字符</h3><p>后缀特殊字符表示的含义如下：</p><p>“*”:  </p><blockquote><p>The node is presently not responding and will not be allocated any new work. If the node remains non-responsive, it will be placed in the DOWN state (except in the case of COMPLETING, DRAINED, DRAINING, FAIL, FAILING nodes).  </p></blockquote><p>表示节点目前无响应，不会将任何作业分配到该节点。如果节点仍然没有响应，它将被系统置为<code>DOWN</code>状态（状态为<code>COMPLETING</code>、<code>DRAINED</code>、<code>DRAINING</code>、<code>RAIL</code>、<code>FAILING</code>的节点除外）  </p><p>“~”:</p><blockquote><p>The node is presently in a power saving mode (typically running at reduced frequency).  </p></blockquote><p>表示该节点处于节能模式下（通常是处于降频工作状态）  </p><p>“#”:</p><blockquote><p>The node is presently being powered up or configured.  </p></blockquote><p>节点正在启动或配置中。  </p><p>“$”:  </p><blockquote><p>The node is currently in a reservation with a flag value of “maintenance”.  </p></blockquote><p>节点正处于标识值为<code>maintenance</code>的保留（预约）状态（详见资源预留相关文档）  </p><p>“@”:  </p><blockquote><p>The node is pending reboot.</p></blockquote><p>节点正在被安排重启。  </p><h3 id="节点状态"><a href="#节点状态" class="headerlink" title="节点状态"></a>节点状态</h3><ul><li>ALLOCATED</li></ul><blockquote><p>The node has been allocated to one or more jobs.  </p></blockquote><p>该节点已被分配给一个或多个作业。  </p><ul><li>ALLOCATED+</li></ul><blockquote><p>The node is allocated to one or more active jobs plus one or more jobs are in the process of COMPLETING.  </p></blockquote><p>该节点已被分配给一个或多个作业，并且其中一部分作业正在完成过程中，即部分作业处于<code>COMPLETING</code>状态。</p><ul><li>COMPLETING</li></ul><blockquote><p>All jobs associated with this node are in the process of COMPLETING. This node state will be removed when all of the job’s processes have terminated and the Slurm epilog program (if any) has terminated. See the Epilog parameter description in the slurm.conf man page for more information.  </p></blockquote><p>与此节点相关联的所有作业都处于完成过程中。Slurm允许用户指定作业完成后要执行的程序(详见slurm.conf文件中的<code>Epilog</code>参数)当所有作业进程以及epilog程序都已终止时，该状态会被移除。</p><ul><li>DOWN</li></ul><blockquote><p>The node is unavailable for use. Slurm can automatically place nodes in this state if some failure occurs. System administrators may also explicitly place nodes in this state. If a node resumes normal operation, Slurm can automatically return it to service. See the ReturnToService and SlurmdTimeout parameter descriptions in the slurm.conf(5) man page for more information.  </p></blockquote><p>节点不可用。如果出现某些故障，Slurm会自动将节点置于此状态，系统管理员也可以手动将节点置于此状态。如果执行了某些恢复操作，Slurm会自动使节点回归服务。相关配置参考slurm.conf中<code>ReturnToService</code>以及<code>SlurmdTimeout</code>参数的配置描述。</p><ul><li>DRAINED</li></ul><blockquote><p>The node is unavailable for use per system administrator request. See the update node command in the scontrol(1) man page or the slurm.conf(5) man page for more information.  </p></blockquote><p>节点不可接受用户请求。</p><ul><li>DRAINING</li></ul><blockquote><p>The node is currently executing a job, but will not be allocated to additional jobs. The node state will be changed to state DRAINED when the last job on it completes.  </p></blockquote><p>节点正在执行作业，但后续不会再分配到任何作业，当节点上最后一个作业执行完毕后，该状态会转变成<code>DRAINED</code>  </p><ul><li>ERROR</li></ul><blockquote><p>The node is currently in an error state and not capable of running any jobs. Slurm can automatically place nodes in this state if some failure occurs. System administrators may also explicitly place nodes in this state. If a node resumes normal operation, Slurm can automatically return it to service. See the ReturnToService and SlurmdTimeout parameter descriptions in the slurm.conf(5) man page for more information.  </p></blockquote><p>节点当前处于出错状态，无法运行任何作业。如果发生某些错误，Slurm会自动将节点置于此状态，系统管理员也可以手动将节点置于此状态。如果执行了某些恢复操作，Slurm会自动使节点回归服务。相关配置参考slurm.conf中<code>ReturnToService</code>以及<code>SlurmdTimeout</code>参数的配置描述。  </p><ul><li>FAIL</li></ul><blockquote><p>The node is expected to fail soon and is unavailable for use per system administrator request.  </p></blockquote><p>节点即将不可用。（与DRAIN相似，但是DRAIN通常是指主动拒绝分配作业到该节点，而FAIL则是无法分配到该节点）</p><ul><li>FAILING</li></ul><blockquote><p>The node is currently executing a job, but is expected to fail soon and is unavailable for use per system administrator request.  </p></blockquote><p>节点正在执行作业，但即将不可用。（与DRAINING很相似，区别在于工作在该节点上的任务将无法完成）</p><ul><li>FUTURE</li></ul><blockquote><p>The node is currently not fully configured, but expected to be available at some point in the indefinite future for use.  </p></blockquote><p>该节点目前尚未完全配置，但预计在无限期的未来能使用。  </p><ul><li>IDLE</li></ul><blockquote><p>The node is not allocated to any jobs and is available for use.  </p></blockquote><p>该节点未分配给任何作业，可供使用。  </p><ul><li>MAINT</li></ul><blockquote><p>The node is currently in a reservation with a flag value of “maintainence”.<br>节点当前处于保留状态，标志值为“维护”。（详见资源预留相关文档）  </p></blockquote><ul><li>REBOOT  </li></ul><blockquote><p>The node is currently scheduled to be rebooted.  </p></blockquote><p>节点正计划重启。</p><ul><li>MIXED</li></ul><blockquote><p>The node has some of its CPUs ALLOCATED while others are IDLE.  </p></blockquote><p>节点资源被部分分配。  </p><ul><li>PERFCTRS (NPC)</li></ul><blockquote><p>Network Performance Counters associated with this node are in use, rendering this node as not usable for any other jobs  </p></blockquote><p>该节点被用于监控网络性能，无法用于其他作业。  </p><ul><li>POWER_DOWN</li></ul><blockquote><p>The node is currently powered down and not capable of running any jobs.  </p></blockquote><p>该节点目前已关闭电源，无法运行任何作业。该状态是由省电模式节能程序设置的，详细<code>SuspendProg</code>和<code>ResumeProg</code>配置。</p><ul><li>POWER_UP</li></ul><blockquote><p>The node is currently in the process of being powered up.  </p></blockquote><p>节点正在开机。该状态是由省电模式节能程序设置的，详细<code>SuspendProg</code>和<code>ResumeProg</code>配置。  </p><ul><li>RESERVED</li></ul><blockquote><p>The node is in an advanced reservation and not generally available.  </p></blockquote><p>节点处于高级预留状态，通常不可用。（详见资源预留配置）。  </p><ul><li>UNKNOWN  </li></ul><blockquote><p>The Slurm controller has just started and the node’s state has not yet been determined.  </p></blockquote><p>Slurm控制器刚刚启动，节点的状态尚未确定。</p><h3 id="状态分类"><a href="#状态分类" class="headerlink" title="状态分类"></a>状态分类</h3><p>虽然状态非常多，但是总的来说我们可以把它分成以下几大类：</p><ul><li>工作中： ALLOCATED   ALLOCATED+   COMPLETING    DRAINING    FAILING  MIXED  </li><li>空闲：  IDLE</li><li>不可用： DOWN    DRAINED   ERROR   FAIL   FUTURE   MAINT   REBOOT   PERFCTRS    POWER_UP   RESERVED   包含- # $ @任一字符</li><li>未知： UNKNOWN</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/sinfo.png&quot; alt=&quot;sinfo&quot;&gt;&lt;/p&gt;
&lt;p&gt;前面介绍了如何查看作业的状态，下面我们介绍一下如何查看节点的状态。跟作业状态查看类似，节点状态也可以通过命令行获取，有两种方式：&lt;code&gt;sinfo&lt;/code&gt;和&lt;code&gt;scontrol show node&lt;/code&gt;，命令行工具的使用时非常简单的，但是节点的状态理解比较复杂。还是先从简单的介绍命令行工具开始吧。
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC 系列文章(10):作业状态</title>
    <link href="http://zxh.site/2018/09/23/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-10-%E4%BD%9C%E4%B8%9A%E7%8A%B6%E6%80%81/"/>
    <id>http://zxh.site/2018/09/23/HPC-系列文章-10-作业状态/</id>
    <published>2018-09-23T11:04:44.000Z</published>
    <updated>2018-10-21T10:57:46.069Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/squeue.png" alt="squeue"></p><p>前面介绍了作业的提交，下面我们介绍一下如何查看作业的状态。查看作业状态的方式有多种，Slurm提供了多个命令行工具，包括scontrol,squeue,sstat,sview等等都可以查看作业状态，不过由于我们暂时还没介绍到交互作业和作业步(sstat)，可视化(sview)也不是我们关心的重点，我们先只介绍scontrol和squeue查看作业状态。</p><p>如果只是讲这两个命令的使用的话，就大可不必单独写一篇文章来赘述了，我们要关注的核心是状态对应的具体含义。不过在开始之前我们还是先简单介绍一下如何使用这两个命令行工具。<a id="more"></a></p><h2 id="命令行工具"><a href="#命令行工具" class="headerlink" title="命令行工具"></a>命令行工具</h2><h3 id="scontrol"><a href="#scontrol" class="headerlink" title="scontrol"></a>scontrol</h3><p>通过<code>scontrol show job [jobid]</code>可以查看作业（或指定jobid的作业）的详细状态。</p><p><img src="http://img.chinahpc.club/2018/09/show_job.png" alt="状态查看"></p><p>可以看到输出了非常多的信息，包括节点状态、优先级、资源需求、作业时间、运行的节点、输出文件等等。如果希望输出的内容更方便解析，可以加上<code>--oneliner</code>参数，则会按行以空格分割的键值对形式输出。</p><p><img src="http://img.chinahpc.club/2018/09/oneliner.png" alt="oneliner"></p><h3 id="squeue"><a href="#squeue" class="headerlink" title="squeue"></a>squeue</h3><p>scontrol输出的信息非常完整，不过缺点是效率较低，如果系统中存在大量的作业，该命令的输出响应速度则会变得非常慢，而有时候我们并不需要那么详细的信息，如果只是查看作业的状态的话，<code>squeue</code>则是更好的选择</p><p><img src="http://img.chinahpc.club/squeue.png" alt="squeue"></p><h2 id="作业状态"><a href="#作业状态" class="headerlink" title="作业状态"></a>作业状态</h2><p>每个作业的执行过程中通常要经历多个状态，典型的作业状态包括PENDING, RUNNING, SUSPENDED, COMPLETING和COMPLETED。</p><h3 id="作业的生命周期"><a href="#作业的生命周期" class="headerlink" title="作业的生命周期"></a>作业的生命周期</h3><p>对于一个作业而言，从提交到结束，有一个状态转换的过程，用流行的说法，我们也可以称之为作业的生命周期。下图是我整理的一个作业状态转换图：</p><p><img src="http://img.chinahpc.club/2018/09/state_convert.png" alt="状态转换图"></p><h3 id="状态对照表"><a href="#状态对照表" class="headerlink" title="状态对照表"></a>状态对照表</h3><p>下面是所有可能出现的作业状态对应的编码以及其含义解释</p><table><thead><tr><th>编码缩写(Short code)</th><th>状态名(State)</th><th>英文含义 (meaning)</th><th>中文翻译</th></tr></thead><tbody><tr><td>BF</td><td>BOOT_FAIL</td><td>Job terminated due to launch failure, typically due to a hardware failure (e.g. unable to boot the node or block and the job can not be requeued).</td><td>作业由于启动失败而终止，通常是由于硬件故障。</td></tr><tr><td>CA</td><td>CANCELLED</td><td>Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated.</td><td>作业被用户或系统管理员终止，该作业可能已经启动或者没有启动。</td></tr><tr><td>CD</td><td>COMPLETED</td><td>Job has terminated all processes on all nodes with an exit code of zero.</td><td>作业包含的每个进程在所有节点上都已结束且没有出现问题。</td></tr><tr><td>CF</td><td>CONFIGURING</td><td>Job has been allocated resources, but are waiting for them to become ready for use (e.g. booting).</td><td>作业已经被分配资源，正在等待所有资源准备就绪。</td></tr><tr><td>CG</td><td>COMPLETING</td><td>Job is in the process of completing. Some processes on some nodes may still be active.</td><td>作业正在完成中。某些节点上的某些进程可能仍然是活动的。</td></tr><tr><td>F</td><td>FAILED</td><td>Job terminated with non-zero exit code or other failure condition.</td><td>非零退出码结束或其他错误导致的作业终止。</td></tr><tr><td>NF</td><td>NODE_FAIL</td><td>Job terminated due to failure of one or more allocated nodes.</td><td>作业由于一个或多个分配的节点的故障而终止。</td></tr><tr><td>PD</td><td>PENDING</td><td>Job is awaiting resource allocation.</td><td>正在等待资源分配。</td></tr><tr><td>PR</td><td>PREEMPTED</td><td>Job terminated due to preemption.</td><td>作业由于资源被抢占而终止。</td></tr><tr><td>RV</td><td>REVOKED</td><td>Sibling was removed from cluster due to other cluster starting the job.</td><td>作业已转移到其他集群上。</td></tr><tr><td>R</td><td>RUNNING</td><td>Job currently has an allocation.</td><td>作业正在运行中。</td></tr><tr><td>SE</td><td>SPECIAL_EXIT</td><td>The job was requeued in a special state. This state can be set by users, typically in EpilogSlurmctld, if the job has terminated with a particular exit value.</td><td>特殊状态下的重新排队，该状态通常是由用户配置的EpilogSlurmctld设置的。</td></tr><tr><td>ST</td><td>STOPPED</td><td>Job has an allocation, but execution has been stopped with SIGSTOP signal. CPUS have been retained by this job.</td><td>作业已经分配运行，但执行过程由于接收到SIGSTOP信号而停止，该作业将继续保有已分配的CPU。</td></tr><tr><td>S</td><td>SUSPENDED</td><td>Job has an allocation, but execution has been suspended and CPUs have been released for other jobs.</td><td>作业在执行过程中被挂起，所分配的资源将被释放用于其他作业。</td></tr><tr><td>TO</td><td>TIMEOUT</td><td>Job terminated upon reaching its time limit.</td><td>工作在达到期限后终止。</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/squeue.png&quot; alt=&quot;squeue&quot;&gt;&lt;/p&gt;
&lt;p&gt;前面介绍了作业的提交，下面我们介绍一下如何查看作业的状态。查看作业状态的方式有多种，Slurm提供了多个命令行工具，包括scontrol,squeue,sstat,sview等等都可以查看作业状态，不过由于我们暂时还没介绍到交互作业和作业步(sstat)，可视化(sview)也不是我们关心的重点，我们先只介绍scontrol和squeue查看作业状态。&lt;/p&gt;
&lt;p&gt;如果只是讲这两个命令的使用的话，就大可不必单独写一篇文章来赘述了，我们要关注的核心是状态对应的具体含义。不过在开始之前我们还是先简单介绍一下如何使用这两个命令行工具。
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC 系列文章(9):指定作业参数</title>
    <link href="http://zxh.site/2018/09/16/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-9-%E6%8C%87%E5%AE%9A%E4%BD%9C%E4%B8%9A%E5%8F%82%E6%95%B0/"/>
    <id>http://zxh.site/2018/09/16/HPC-系列文章-9-指定作业参数/</id>
    <published>2018-09-16T09:36:35.000Z</published>
    <updated>2018-10-21T10:57:46.069Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/2018/09/slurm-script.png" alt="parameter"></p><p>上一节中我们介绍了最简单的作业提交，我们将脚本编辑好之后交由Slurm调度，然后该脚本会运行在合适的节点上，并得到输出结果。在单台计算机上运行时，我们会觉得一切都很自然，就像是写几行代码然后编译运行得到结果一样。不过当你有一个集群时，就有一些区别了。假设我们有一个20个计算节点的集群，假设是如下配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">################################################</span><br><span class="line">#           Nodes Configurations           #</span><br><span class="line">################################################</span><br><span class="line">NodeName=node[01-10] Sockets=2 CoresPerSocket=32 ThreadsPerCore=1 Gres=gpu:tesla:no_consume:2 RealMemory=65535 TmpDisk=100000 Weight=6 Feature=video</span><br><span class="line">NodeName=node[11-20] CPUs=4 RealMemory=8192 TmpDisk=14190</span><br><span class="line">#</span><br><span class="line">################################################</span><br><span class="line">#           Partition Configurations           #</span><br><span class="line">################################################</span><br><span class="line">PartitionName=node-all Nodes=node[01-20] State=UP Default=YES MaxTime=60</span><br><span class="line">PartitionName=performance MaxTime=UNLIMITED Nodes=node[01-10] State=UP</span><br><span class="line">PartitionName=common MaxTime=UNLIMITED Nodes=node[11-20] State=UP</span><br></pre></td></tr></table></figure><p>一共有20个节点，我将他们分成了三个队列：高性能、普通、二者混合。<a id="more"></a></p><div class="note info"><p>从上面可以看到同一个节点是允许同时出现在多个分区中的，分区只是逻辑上的一种组合。</p></div><h2 id="指定参数"><a href="#指定参数" class="headerlink" title="指定参数"></a>指定参数</h2><p>既然我们有那么多节点，我们如何把作业提交到我们想要的某些或者某个节点上呢？我们能否指定作业的优先级、运行时长和各种运行策略呢？这就要结合上一节讲到的节点/队列配置和接下来我们要讲的作业参数的指定。</p><h3 id="通过命令行参数"><a href="#通过命令行参数" class="headerlink" title="通过命令行参数"></a>通过命令行参数</h3><p>假设我们的某个作业需要3个节点同时计算，并且每个节点的内存不低于8G，并且希望运行在高性能的节点上。那么我们可以这样写：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbatch --nodes=3 --mem=8192 --partiton=performance myjob.sh</span><br></pre></td></tr></table></figure><p>我们来看一个更复杂的例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbatch --nodes=5 --mem=8192 --partion=node-all --nodelist=node01,node02,node03,node11,node20 --cpus-per-task=4 myjob.sh</span><br></pre></td></tr></table></figure><p>上面的命令翻译一下就是，需要5个节点，每个需要4核处理器，8G内存，指定运行在node01,node02,node03,node11,node20这五个节点上运行。</p><p>我们可以指定更多更详细的参数，包括处理器、内存、通用资源（GPU/IB网络/共享存储等）、License、优先级、预约资源等等，除此之外我们还可以指定开始时间、运行时长、作业名、输出文件等等。更多的选项可以参考文档：<a href="https://slurm.schedmd.com/sbatch.html" target="_blank" rel="noopener">sbatch</a>进行了解。</p><h3 id="通过脚本注释指定参数"><a href="#通过脚本注释指定参数" class="headerlink" title="通过脚本注释指定参数"></a>通过脚本注释指定参数</h3><p>在命令行中指定参数很简单方便，不过不方便重复运行。Slurm还提供了另一种作业参数指定方式，将参数以注释的形式写到脚本中，Slurm会自动解析这些参数。</p><p><img src="http://img.chinahpc.club/2018/09/slurm-script.png" alt="parameter"></p><h2 id="常见误区"><a href="#常见误区" class="headerlink" title="常见误区"></a>常见误区</h2><p>曾有人问我：我有4台8核处理器的服务器，为什么我提交一个需要16核的任务的时候，作业永远不会运行？</p><p>其实这是很多人对调度系统的一个错误认识，我们必须清楚计算单元并不会自动融合，调度系统也并不会自动拆分作业，这是学习调度系统之前应该了解的要点。我们可以把集群具体的计算资源认为是用户无感知的，也就是说用户无需关心具体跑在某个服务器上，<strong>但并不是所有的计算资源可以看作是一台机器。</strong></p><p>大家小学应该都学过乌鸦喝水的故事，这里我举一个例子，我们把集群中单台服务器看作是一个瓶子，作业看作是一颗一颗不规则的石头，集群则是一大堆瓶子，我们可以得到以下简单的结论：</p><ul><li>石头尺寸是大小不一的（每个作业对硬件资源的需求不一样，需要16核的作业就是大石头，只需要1核的就是小石头）</li><li>一块石头显然没有办法同时放在两个瓶子里（一个作业不能同时部分运行在两台服务器上）</li><li>我们希望瓶子盛放尽可能多的石头（调度系统的作用是将作业放到能放下的服务器上，并且尽可能的多放）</li><li>当石头的尺寸超过任意的瓶子所能容纳的体积时，我们无法将石头放入瓶子内（16核需求的作业无法放到任意一台8核的服务器上）</li><li>大石头不会自动变成小石头，除非我们主动敲碎它（调度系统不会为你自动将一个作业拆成两个小作业）。</li></ul><p>回到上面那个问题，我们该如何处理呢？答案是要么你将作业需求变得更小，改成两个并行作业，每个只需要8核处理器。要么就往系统中添加一台16核以上配置的服务器。至于具体怎么做这不是本节要讨论的，后面再做介绍。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这一节我们介绍了如何为作业指定参数，我们可以通过命令行参数或者将参数编写在作业脚本中两种不同的方式来指定作业参数。后面我们将进一步介绍一些高级的用法。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/2018/09/slurm-script.png&quot; alt=&quot;parameter&quot;&gt;&lt;/p&gt;
&lt;p&gt;上一节中我们介绍了最简单的作业提交，我们将脚本编辑好之后交由Slurm调度，然后该脚本会运行在合适的节点上，并得到输出结果。在单台计算机上运行时，我们会觉得一切都很自然，就像是写几行代码然后编译运行得到结果一样。不过当你有一个集群时，就有一些区别了。假设我们有一个20个计算节点的集群，假设是如下配置：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;################################################&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#           Nodes Configurations           #&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;################################################&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;NodeName=node[01-10] Sockets=2 CoresPerSocket=32 ThreadsPerCore=1 Gres=gpu:tesla:no_consume:2 RealMemory=65535 TmpDisk=100000 Weight=6 Feature=video&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;NodeName=node[11-20] CPUs=4 RealMemory=8192 TmpDisk=14190&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;################################################&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#           Partition Configurations           #&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;################################################&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;PartitionName=node-all Nodes=node[01-20] State=UP Default=YES MaxTime=60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;PartitionName=performance MaxTime=UNLIMITED Nodes=node[01-10] State=UP&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;PartitionName=common MaxTime=UNLIMITED Nodes=node[11-20] State=UP&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;一共有20个节点，我将他们分成了三个队列：高性能、普通、二者混合。
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC 系列文章(8):第一个Slurm作业</title>
    <link href="http://zxh.site/2018/09/08/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-8-%E7%AC%AC%E4%B8%80%E4%B8%AASlurm%E4%BD%9C%E4%B8%9A/"/>
    <id>http://zxh.site/2018/09/08/HPC-系列文章-8-第一个Slurm作业/</id>
    <published>2018-09-08T10:00:04.000Z</published>
    <updated>2018-10-21T10:57:46.070Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/2018/08/concept.gif" alt="concept"></p><p>我们已经搭建好了一个简单的Slurm环境，包含两个计算节点和一个管理节点。接下来我们将尝试往作业系统中提交作业。在这之前我们已经在<a href="http://blog.zxh.site/2018/08/11/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-4-Slurm%E6%A6%82%E8%A7%88/#more" target="_blank" rel="noopener">HPC 系列文章(4):Slurm概览</a>中介绍了包括节点、分区、作业等一些基本概念，在提交作业之前，我们进一步的学习一下这几个概念。</p><p>介绍节点和分区的概念，我们先从配置文件中简单了解一下它们，这里我以一个自己编辑的范例配置来介绍，我们先只看节点和分区的部分：<a id="more"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#</span><br><span class="line"># Sample /etc/slurm.conf</span><br><span class="line">################################################</span><br><span class="line">#           SCHEDULING &amp; ALLOCATION            #</span><br><span class="line">################################################</span><br><span class="line">FastSchedule=2</span><br><span class="line">#</span><br><span class="line">################################################</span><br><span class="line">#           Nodes Configurations           #</span><br><span class="line">################################################</span><br><span class="line">NodeName=master Sockets=2 CoresPerSocket=8 ThreadsPerCore=1 Gres=gpu:tesla:no_consume:2 RealMemory=65535 TmpDisk=100000 Weight=6 Feature=video</span><br><span class="line">NodeName=node01 CPUs=8 RealMemory=4096 TmpDisk=14190</span><br><span class="line">#</span><br><span class="line">################################################</span><br><span class="line">#           Partition Configurations           #</span><br><span class="line">################################################</span><br><span class="line">PartitionName=node-all Nodes=master,node01 State=UP Default=YES MaxTime=60</span><br><span class="line">PartitionName=master-only MaxTime=UNLIMITED Nodes=master State=UP</span><br></pre></td></tr></table></figure><h2 id="节点"><a href="#节点" class="headerlink" title="节点"></a>节点</h2><p>先说节点，对于Slurm系统而言，每一台安装并正确配置了slurmd进程的服务器就是一个节点。安装了slurmctld的节点虽然我们称之为主控节点，但实际上它并不是我们这里讨论的节点，这里指的节点即计算资源。为了使节点的概念更符合我们的直觉，我们可以认为每个计算节点就是一台服务器。</p><h3 id="节点配置"><a href="#节点配置" class="headerlink" title="节点配置"></a>节点配置</h3><p>回到配置文件中，我们先看master的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NodeName=master Sockets=2 CoresPerSocket=8 ThreadsPerCore=1 RealMemory=65535 TmpDisk=100000 Weight=6 Gres=gpu:tesla:no_consume:2 Feature=video</span><br></pre></td></tr></table></figure><p>每一项简单介绍一下：</p><ul><li>NodeName=master 即节点名为master</li><li>Sockets=2 CPU插槽两个</li><li>CoresPerSocket=8 每个插槽上CPU的核心数量，这里即8核处理器</li><li>ThreadsPerCore=1 单个物理核心中的逻辑线程数</li></ul><div class="note info"><p>CPU数量计算公式：CPUs = Sockets*CoresPerSocket*ThreadsPerCore，上面master包含16个逻辑CPU,也可以直接通过CPUs来设定逻辑CPU数量，上面node01就直接采用了CPUs配置项。</p></div><ul><li>RealMemory=65535 节点上实内存的大小，以兆字节为单位，即该节点内存为64G</li><li>TmpDisk=100000 临时磁盘存储的总大小，以兆字节为单位，该节点有100G临时存储空间</li><li>Weight=6 节点权重为6，在所有条件相同的情况下，作业将被分配到满足其要求的最低权重的节点。</li><li>Gres=gpu:tesla:no_consume:2 GRES是Slurm中的通用资源配置项，属于比较高级的配置项，这个留到后面再仔细介绍，这里只需要知道根据配置文件显示该节点配备了两块Tesla的GPU</li><li>Feature=video 以逗号分隔的任意字符串列表，表示与节点相关的某些特征。简单的说就是给节点打的tag。</li></ul><p>关于节点的配置项其实还有很多，更多内容可以参考<a href="https://slurm.schedmd.com/slurm.conf.html" target="_blank" rel="noopener">slurm.conf</a>，这里我们先就介绍这些。</p><p>为什么要配置这些东西呢？原因很简单，调度系统的核心功能说白了就是帮你把作业分配到最合适的节点上运行。什么是合适的节点呢？能满足你作业需求并且符合当前分配策略的最优解就是合适的节点。你为节点和作业配置的参数越详尽，越能精细化分配条件。</p><h3 id="配置我可以瞎写吗？"><a href="#配置我可以瞎写吗？" class="headerlink" title="配置我可以瞎写吗？"></a>配置我可以瞎写吗？</h3><p>答案是可以，我在配置文件中设置了FastSchedule=2，该设置项一共有三个有效值(0,1,2)，0则会严格检查节点的实际配置，1会使用配置文件中的值作为配置但会将实际低于配置值的节点自动设置为DRAIN状态（表示资源耗尽，该状态的节点不会接受任何作业），而2则完全不校验实际配置，你想怎么配置就怎么配置，这可以方便我们超配，也方便测试学习。</p><h2 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h2><p>分区将一组节点组合成逻辑上的一个集合，也有人称之为“队列”，实际上节点间并没有队列的概念，所以我认为“分区”是一个更合适的翻译。</p><h3 id="分区配置"><a href="#分区配置" class="headerlink" title="分区配置"></a>分区配置</h3><p>回到配置文件中，我配置了两个队列，分别是node-all包含了master和node01节点，master-only则只包含master节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PartitionName=node-all Nodes=master,node01 State=UP Default=YES MaxTime=60</span><br><span class="line">PartitionName=master-only MaxTime=UNLIMITED Nodes=master State=UP</span><br></pre></td></tr></table></figure><p>以node-all为例简单介绍一下各个配置项：</p><ul><li>PartitionName=node-all 分区名</li><li>Nodes=master,node01 包含的节点，可以用逗号分隔开，也可以使用类似node[01-05]这样的批量配置。</li><li>State=UP 队列的状态</li><li>Default=YES 是否为默认分区，如果没有给作业指明分区，则会分配到默认分区中。</li><li>MaxTime=60 作业的最大运行时间限制。支持多种格式，默认为分钟。这里表示该队列中运行的作业最多可以运行60分钟。</li></ul><p>关于分区的配置项更多内容可以参考<a href="https://slurm.schedmd.com/slurm.conf.html" target="_blank" rel="noopener">slurm.conf</a>，后面我们也会更深入的去讲解一些高级的配置。</p><h2 id="作业提交"><a href="#作业提交" class="headerlink" title="作业提交"></a>作业提交</h2><p><strong>特定时间为用户进行的一次资源申请或分配即可看作一个作业</strong>。这和我们惯性思维中的作业概念并不一致，传统意义上我们总是认为作业应该是某个运行的脚本或者程序，但事实上Slurm的作业只代表一次资源申请或分配。理解这个区别将有利于你理解Slurm中一些比较高级的用法。</p><p>Slurm中有多种作业提交方式，分别是sbatch、srun和salloc，关于它们的区别和用法我们留到后面仔细讲解，这里我们就以sbatch为例提交作业。你需要将你要运行的内容编辑成脚本，然后由sbatch命令指定参数和该脚本。我们先编辑一个最简单的bash脚本命名为<code>test.sh</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Hello world"</span></span><br><span class="line">date</span><br><span class="line">hostname</span><br><span class="line">sleep 60</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Bye"</span></span><br></pre></td></tr></table></figure><p>该脚本会依次输出”Hello world”、当前时间、节点名，休眠60秒后会输出Bye，我们试着提交它：</p><p><code>sbatch -p master-only test.sh</code></p><p>一切正常的话，你将得到一个slurm-1.out文件，其中包含了上述输出。你已经成功的提交了一个脚本并且slurm将其分配到了master节点上运行。不过似乎没有体现出调度系统的优势？下一节我们将继续探讨作业提交。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/2018/08/concept.gif&quot; alt=&quot;concept&quot;&gt;&lt;/p&gt;
&lt;p&gt;我们已经搭建好了一个简单的Slurm环境，包含两个计算节点和一个管理节点。接下来我们将尝试往作业系统中提交作业。在这之前我们已经在&lt;a href=&quot;http://blog.zxh.site/2018/08/11/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-4-Slurm%E6%A6%82%E8%A7%88/#more&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;HPC 系列文章(4):Slurm概览&lt;/a&gt;中介绍了包括节点、分区、作业等一些基本概念，在提交作业之前，我们进一步的学习一下这几个概念。&lt;/p&gt;
&lt;p&gt;介绍节点和分区的概念，我们先从配置文件中简单了解一下它们，这里我以一个自己编辑的范例配置来介绍，我们先只看节点和分区的部分：
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC 系列文章(7):Debian/Ubuntu安装Slurm</title>
    <link href="http://zxh.site/2018/09/02/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-7-Debian-Ubuntu%E5%AE%89%E8%A3%85Slurm/"/>
    <id>http://zxh.site/2018/09/02/HPC-系列文章-7-Debian-Ubuntu安装Slurm/</id>
    <published>2018-09-02T00:11:07.000Z</published>
    <updated>2018-10-21T10:57:46.070Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/2018/08/universal_install_script.png" alt="deploy"></p><p>上一篇文章中简单介绍了CentOS/Redhat上如何部署Slurm，这里我们再介绍一下在Debian或Ubuntu上如何正确的安装Slurm。这里我们以Debian 8.5(Jessie)为例，其他更新的版本也类似。</p><h2 id="通过包管理器安装"><a href="#通过包管理器安装" class="headerlink" title="通过包管理器安装"></a>通过包管理器安装</h2><p>最方便的方式是通过包管理器安装，不过apt源中只有较老的版本，通过软件包管理器安装的过程也比较简单，<br>可以直接参考这篇文档：<a href="http://wiki.sc3.uis.edu.co/index.php/Slurm_Installation_on_Debian" target="_blank" rel="noopener">Slurm Installation on Debian</a>，我们还是主要介绍如何安装最新版本的Slurm，为了简化流程，我们将在一台机器上部署所有的服务，该节点即是管理节点也是计算节点。<a id="more"></a></p><h2 id="源码安装"><a href="#源码安装" class="headerlink" title="源码安装"></a>源码安装</h2><h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install hwloc libhwloc-dev libmunge-dev libmunge2 munge mariadb-server libmysqlclient-dev</span><br></pre></td></tr></table></figure><div class="note warning"><p><code>libmysqlclient-dev</code>若缺失会导致slurmdbd找不到<code>accounting_storage_mysql.so</code>，<code>libhwloc-dev</code>缺失则会导致cgroup plugin无法使用。</p></div><h3 id="安装Slurm"><a href="#安装Slurm" class="headerlink" title="安装Slurm"></a>安装Slurm</h3><p>在官网下载最新的安装包，并解压：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://download.schedmd.com/slurm/slurm-17.11.9-2.tar.bz2</span><br><span class="line">tar --bzip -x -f  slurm-17.11.9-2.tar.bz2</span><br></pre></td></tr></table></figure><p>编译并安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> slurm-17.11.9-2</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><div class="note info"><p>注意：与CentOS上直接用yum安装不同的是Debian上编译安装的Slurm默认配置文件路径为<code>/usr/local/etc</code>，默认lib路径为<code>/usr/local/lib/slurm/</code></p></div><p>最后将<code>slurm-17.11.9-2/etc</code>文件夹中的三个service启动脚本拷贝到<code>/etc/systemd/system/</code>目录下</p><h3 id="配置修改"><a href="#配置修改" class="headerlink" title="配置修改"></a>配置修改</h3><p>在<code>slurm-17.11.9-2/etc</code>文件夹中包含了可能用到的各个配置文件的范例，将他们复制到/usr/local/etc下根据实际情况修改即可，这里提供一下我自己的配置文件：</p><ul><li><a href="http://img.chinahpc.club/2018/09/slurm.conf" target="_blank" rel="noopener">slurm.conf</a></li><li><a href="http://img.chinahpc.club/2018/09/slurmdbd.conf" target="_blank" rel="noopener">slurmdbd.conf</a></li><li><a href="http://img.chinahpc.club/2018/09/cgroup.conf" target="_blank" rel="noopener">cgroup.conf</a></li><li><a href="http://img.chinahpc.club/2018/09/cgroup_allowed_devices_file.conf" target="_blank" rel="noopener">cgroup_allowed_devices_file.conf</a></li></ul><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>在任意节点执行sinfo命令，你将看到类似输出，则证明服务工作正常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PARTITION    AVAIL  TIMELIMIT  NODES  STATE NODELIST</span><br><span class="line">master-only*    up   infinite      1   idle debian</span><br></pre></td></tr></table></figure><p>否则请查看/var/log/slurm/slurmctld.log及/var/log/slurm/slurmd.log日志文件排错，下面列出了一些我安装过程中遇到的问题。</p><h3 id="Troubleshoot"><a href="#Troubleshoot" class="headerlink" title="Troubleshoot"></a>Troubleshoot</h3><h4 id="ConstrainRAMSpace-yes无效"><a href="#ConstrainRAMSpace-yes无效" class="headerlink" title="ConstrainRAMSpace=yes无效"></a>ConstrainRAMSpace=yes无效</h4><p>提示ConstrainRAMSpace=yes无效，需要启用cgroup_enable=memory并重启系统，具体方法：</p><ol><li>vim /etc/default/grub</li><li>在GRUB_CMDLINE_LINUX_DEFAULT中补充cgroup_enable=memory swapaccount=1</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># If you change this file, run &apos;update-grub&apos; afterwards to update</span><br><span class="line"># /boot/grub/grub.cfg.</span><br><span class="line"># For full documentation of the options in this file, see:</span><br><span class="line">#   info -f grub -n &apos;Simple configuration&apos;</span><br><span class="line"></span><br><span class="line">GRUB_DEFAULT=0</span><br><span class="line">GRUB_TIMEOUT=5</span><br><span class="line">GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian`</span><br><span class="line">GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet cgroup_enable=memory swapaccount=1&quot;</span><br><span class="line">GRUB_CMDLINE_LINUX=&quot;initrd=/install/initrd.gz&quot;</span><br></pre></td></tr></table></figure><h4 id="slurmdbd无法启动"><a href="#slurmdbd无法启动" class="headerlink" title="slurmdbd无法启动"></a>slurmdbd无法启动</h4><p>提示找不到accounting_storage_mysql.so，是由于编译前未安装<code>libmysqlclient-dev</code>，安装之后重新执行编译安装步骤即可</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/2018/08/universal_install_script.png&quot; alt=&quot;deploy&quot;&gt;&lt;/p&gt;
&lt;p&gt;上一篇文章中简单介绍了CentOS/Redhat上如何部署Slurm，这里我们再介绍一下在Debian或Ubuntu上如何正确的安装Slurm。这里我们以Debian 8.5(Jessie)为例，其他更新的版本也类似。&lt;/p&gt;
&lt;h2 id=&quot;通过包管理器安装&quot;&gt;&lt;a href=&quot;#通过包管理器安装&quot; class=&quot;headerlink&quot; title=&quot;通过包管理器安装&quot;&gt;&lt;/a&gt;通过包管理器安装&lt;/h2&gt;&lt;p&gt;最方便的方式是通过包管理器安装，不过apt源中只有较老的版本，通过软件包管理器安装的过程也比较简单，&lt;br&gt;可以直接参考这篇文档：&lt;a href=&quot;http://wiki.sc3.uis.edu.co/index.php/Slurm_Installation_on_Debian&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Slurm Installation on Debian
&lt;/a&gt;，我们还是主要介绍如何安装最新版本的Slurm，为了简化流程，我们将在一台机器上部署所有的服务，该节点即是管理节点也是计算节点。
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC 系列文章(6):Slurm部署</title>
    <link href="http://zxh.site/2018/08/26/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-6-Slurm%E9%83%A8%E7%BD%B2/"/>
    <id>http://zxh.site/2018/08/26/HPC-系列文章-6-Slurm部署/</id>
    <published>2018-08-26T11:25:51.000Z</published>
    <updated>2018-10-21T10:57:45.881Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/2018/08/universal_install_script.png" alt="deploy"></p><p>本节将介绍Slurm实验环境的安装，包括一个控制节点和两个计算节点（控制节点同时作为计算节点），无特殊说明的情况下一下步骤在每个节点上都要执行（仅有少量区别），如果节点比较多可以考虑使用Ansible或者Salt之类的运维管理工具批量操作。</p><h2 id="安装准备"><a href="#安装准备" class="headerlink" title="安装准备"></a>安装准备</h2><h3 id="关闭SELinux"><a href="#关闭SELinux" class="headerlink" title="关闭SELinux"></a>关闭SELinux</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/selinux</span><br><span class="line">配置 SELINUX=disabled</span><br><span class="line">然后重启</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="关闭部分服务"><a href="#关闭部分服务" class="headerlink" title="关闭部分服务"></a>关闭部分服务</h3><p>建议先将防火墙和NetworkManager服务关掉，后面我们再介绍防火墙的配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld NetworkManager</span><br><span class="line">systemctl disable firewalld NetworkManager</span><br></pre></td></tr></table></figure><h3 id="EPEL源配置"><a href="#EPEL源配置" class="headerlink" title="EPEL源配置"></a>EPEL源配置</h3><p>Slurm所依赖的软件包在EPEL源中才能找到，所以我们需要先配置EPEL</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -Uvh http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span><br></pre></td></tr></table></figure><h3 id="安装时间同步服务"><a href="#安装时间同步服务" class="headerlink" title="安装时间同步服务"></a>安装时间同步服务</h3><p>各个节点上通过ntpd服务实现时间同步，确保各节点的系统时间一致。先校准主控节点的时间，然后配置ntpd服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum install ntp -y</span><br><span class="line">systemctl enable ntpd.service</span><br><span class="line">systemctl start ntpd</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 计算节点上将ntpd服务指向管理节点</span></span><br></pre></td></tr></table></figure><h2 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h2><h3 id="依赖包安装"><a href="#依赖包安装" class="headerlink" title="依赖包安装"></a>依赖包安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y readline-devel perl-ExtUtils-MakeMaker openssl-devel pam-devel rpm-build perl-DBI perl-Switch mariadb-devel hwloc-devel</span><br></pre></td></tr></table></figure><div class="note info"><p>hwloc-devel缺失会导致cgroup plugin无法使用</p></div><h3 id="安装MUNGE"><a href="#安装MUNGE" class="headerlink" title="安装MUNGE"></a>安装MUNGE</h3><p>Slurm采用<a href="https://dun.github.io/munge/" target="_blank" rel="noopener">MUNGE</a>提供进程鉴权服务，集群中每个节点都应该采用相同的密钥。在启动Slurm服务之前，需要先启动munged。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y munge-devel munge-libs munge</span><br></pre></td></tr></table></figure><h4 id="设置正确的权限"><a href="#设置正确的权限" class="headerlink" title="设置正确的权限"></a>设置正确的权限</h4><p>配置正确的文件权限，否则munged服务将无法启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chmod -Rf 700 /etc/munge</span><br><span class="line">chmod -Rf 711 /var/lib/munge</span><br><span class="line">chmod -Rf 700 /var/log/munge</span><br><span class="line">chmod -Rf 0755 /var/run/munge</span><br></pre></td></tr></table></figure><div class="note warning"><p>另外需要注意的是检查一下 / etc/munge/munge.key 文件，文件的 owner 和 group 必须是 munge，否则启动会失败。</p></div><h3 id="下载Slurm软件包"><a href="#下载Slurm软件包" class="headerlink" title="下载Slurm软件包"></a>下载Slurm软件包</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://download.schedmd.com/slurm/slurm-17.11.9-2.tar.bz2</span><br></pre></td></tr></table></figure><h3 id="编译Slurm"><a href="#编译Slurm" class="headerlink" title="编译Slurm"></a>编译Slurm</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpmbuild -ta slurm-17.11.9-2.tar.bz2</span><br></pre></td></tr></table></figure><h3 id="安装Slurm"><a href="#安装Slurm" class="headerlink" title="安装Slurm"></a>安装Slurm</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls -l ~/rpmbuild/RPMS/x86_64/*.rpm</span><br><span class="line">rpm -Uvh ~/rpmbuild/RPMS/x86_64/*.rpm</span><br></pre></td></tr></table></figure><div class="note info"><p>在计算节点上无需安装slurmdbd、slurmctld服务</p></div><h3 id="安装MariaDB"><a href="#安装MariaDB" class="headerlink" title="安装MariaDB"></a>安装MariaDB</h3><p>因为Slurmdbd依赖MariaDB所以如果需要使用Slurmdbd则我们需要安装MariaDB</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-server -y</span><br><span class="line">systemctl start mariadb</span><br><span class="line">systemctl enable mariadb</span><br><span class="line">mysql_secure_installation</span><br></pre></td></tr></table></figure><h4 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h4><p>使用mysql命令行执行如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> grant all on slurm_acct_db.* TO <span class="string">'slurm'</span>@<span class="string">'localhost'</span></span></span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> identified by <span class="string">'some_pass'</span> with grant option;</span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> create database slurm_acct_db;</span></span><br></pre></td></tr></table></figure><h2 id="Slurm基本配置"><a href="#Slurm基本配置" class="headerlink" title="Slurm基本配置"></a>Slurm基本配置</h2><p>进入 / etc/slurm / 目录，复制 slurm.conf.example 文件成 slurm.conf，然后编辑 / etc/slurm/slurm.conf 文件，下面是我的文件中修改的部分：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ControlMachine=master</span><br><span class="line">ControlAddr=10.0.0.21</span><br><span class="line">SlurmUser=slurm</span><br><span class="line">SelectType=select/cons_res</span><br><span class="line">SelectTypeParameters=CR_Core</span><br><span class="line">FastSchedule = 2</span><br><span class="line">NodeName=master RealMemory=2048 Sockets=1 CoresPerSocket=2 State=IDLE</span><br><span class="line">NodeName=node01 RealMemory=1024 Sockets=1 CoresPerSocket=1 State=IDLE</span><br><span class="line">PartitionName=all Nodes=master,node01 Default=YES State=UP</span><br><span class="line">PartitionName=node01 Nodes=node01 Default=NO</span><br></pre></td></tr></table></figure><p>这是一份最简单的Slurm配置，后面我们将逐步详细介绍Slurm的配置项。将此配置项拷贝到node01的相同目录下。启动服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 主控节点</span></span><br><span class="line">systemctl start slurmctld slurmd</span><br><span class="line">systemctl enable slurmctld slurmd</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 计算节点</span></span><br><span class="line">systemctl start slurmd</span><br><span class="line">systemctl enable slurmd</span><br></pre></td></tr></table></figure><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>在任意节点执行sinfo命令，你将看到类似输出，则证明服务工作正常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> sinfo</span><br><span class="line">PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST</span><br><span class="line">all*      up     infinite      2   idle  master,node01</span><br><span class="line">node01    up     infinite      1   idle  node01</span><br></pre></td></tr></table></figure><p>否则请查看/var/log/slurm/slurmctld.log及/var/log/slurm/slurmd.log日志文件排错。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/2018/08/universal_install_script.png&quot; alt=&quot;deploy&quot;&gt;&lt;/p&gt;
&lt;p&gt;本节将介绍Slurm实验环境的安装，包括一个控制节点和两个计算节点（控制节点同时作为计算节点），无特殊说明的情况下一下步骤在每个节点上都要执行（仅有少量区别），如果节点比较多可以考虑使用Ansible或者Salt之类的运维管理工具批量操作。&lt;/p&gt;
&lt;h2 id=&quot;安装准备&quot;&gt;&lt;a href=&quot;#安装准备&quot; class=&quot;headerlink&quot; title=&quot;安装准备&quot;&gt;&lt;/a&gt;安装准备&lt;/h2&gt;&lt;h3 id=&quot;关闭SELinux&quot;&gt;&lt;a href=&quot;#关闭SELinux&quot; class=&quot;headerlink&quot; title=&quot;关闭SELinux&quot;&gt;&lt;/a&gt;关闭SELinux&lt;/h3&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;vim /etc/sysconfig/selinux&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;配置 SELINUX=disabled&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;然后重启&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC 系列文章(5):搭建实验环境</title>
    <link href="http://zxh.site/2018/08/19/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-5-%E6%90%AD%E5%BB%BA%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83/"/>
    <id>http://zxh.site/2018/08/19/HPC-系列文章-5-搭建实验环境/</id>
    <published>2018-08-19T05:47:22.000Z</published>
    <updated>2018-10-21T10:57:46.070Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/uploads/2016/05/slurm.gif" alt="deploy"></p><p>正所谓纸上得来终觉浅，绝知此事要躬行，经过前面的一些简单的理论知识学习之后，我们要动手进行实验，在进一步学习之前，我们需要安装一个HPC实验环境。安装的过程中我们会对Slurm的架构和配置有更深入的了解，为了降低门槛，我们的实验环境采用尽量简单的部署方式。<a id="more"></a></p><h2 id="部署方案"><a href="#部署方案" class="headerlink" title="部署方案"></a>部署方案</h2><h3 id="节点组成"><a href="#节点组成" class="headerlink" title="节点组成"></a>节点组成</h3><p>HPC调度系统是一套分布式的系统，包含一系列服务，在生产环境中我们通常会按照节点所提供的功能将节点进行逻辑划分，通常我们可以按下面的方式进行划分：</p><ol><li>控制节点：控制节点负责计算节点的管理并提供账户管理等功能，其上运行的服务包括：调度管理控制器，例如Slurmctld、监控服务主控端例如gmetad、账户管理服务例如ypserv，除此之外包括数据库、消息队列、时间同步等服务都可以安装在控制节点上。</li><li>计算节点：计算节点负责运行具体的计算任务，主要运行调度系统的agent端，例如slurmd。</li><li>I/O节点：为了提供更好的性能，HPC环境中通常会配备高性能的集中存储，这些节点通常使用高性能的磁盘阵列部署分布式文件系统，例如Lustre/Gluster等</li><li>登陆节点：通常不允许用户直接登陆控制节点或计算节点，用户应该使用登陆节点进行作业提交。</li></ol><p><img src="http://img.chinahpc.club/2018/08/deploy.png" alt="deploy"></p><p>以上几类节点是从功能上进行的区分，但并非固定的方案，实际部署时我们可以根据需求灵活配置，例如将NIS/LDAP之类的身份管理服务部署在独立的机器上，将数据库和消息队列配置多机高可用等。总的来说为了提高系统的可用性和高性能，我们可以将服务部署在若干台物理机上各司其职，而如果出于学习研究的目的，我们也可以将所有服务安装到一起。</p><h3 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h3><p>在生产环节中Slurm通常被安装在物理服务器上以提供最佳的性能，但为了方面个人学习，我建议大家可以使用虚拟机进行安装。这里我们采用VirtualBox运行两个虚拟机进行环境搭建，其中管理节点同时也作为计算节点，具体配置如下：</p><ul><li>操作系统：CentOS 7.2+</li><li>VCPU：<ul><li>管理节点：2核</li><li>计算节点：1核</li></ul></li><li>内存：<ul><li>管理节点2G+</li><li>计算节点2G+</li></ul></li><li>磁盘：<ul><li>不低于10G</li></ul></li><li>网络：<ul><li>管理节点：<ul><li>桥接网络：192.168.1.0/24</li><li>内部网络：10.0.0.0/8</li></ul></li><li>计算节点：<ul><li>内部网络：10.0.0.0/8</li></ul></li></ul></li><li>共享存储： NFS</li><li>账户管理： NIS</li><li>调度系统： Slurm</li><li>监控系统： Ganglia</li></ul><div class="note info"><p>以上配置为最低配置，如果条件允许可以根据情况自行添加更多资源</p></div><h4 id="服务图示"><a href="#服务图示" class="headerlink" title="服务图示"></a>服务图示</h4><p><img src="http://img.chinahpc.club/2018/08/nodes-services.png" alt="service"></p><h4 id="网络图示"><a href="#网络图示" class="headerlink" title="网络图示"></a>网络图示</h4><p><img src="http://img.chinahpc.club/2018/08/nodes-network.png" alt="network"></p><p>下一节开始部署具体的服务。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/uploads/2016/05/slurm.gif&quot; alt=&quot;deploy&quot;&gt;&lt;/p&gt;
&lt;p&gt;正所谓纸上得来终觉浅，绝知此事要躬行，经过前面的一些简单的理论知识学习之后，我们要动手进行实验，在进一步学习之前，我们需要安装一个HPC实验环境。安装的过程中我们会对Slurm的架构和配置有更深入的了解，为了降低门槛，我们的实验环境采用尽量简单的部署方式。
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC 系列文章(4):Slurm概览</title>
    <link href="http://zxh.site/2018/08/11/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-4-Slurm%E6%A6%82%E8%A7%88/"/>
    <id>http://zxh.site/2018/08/11/HPC-系列文章-4-Slurm概览/</id>
    <published>2018-08-11T10:25:31.000Z</published>
    <updated>2018-10-21T10:57:46.070Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/2018/08/slurm-better-thumbnail.png" alt="Cover"></p><p>经过前一篇文章的学习，大家应该对HPC集群调度系统有了初步的认识。接下来我们将以Slurm作为调度系统进行HPC作业相关的学习，会深入介绍Slurm的架构、配置及使用。</p><h2 id="Slurm概述"><a href="#Slurm概述" class="headerlink" title="Slurm概述"></a>Slurm概述</h2><p>Slurm是一个可工作于各种不同规模的Linux集群之上的开源、具备容错性和高度可扩展性的集群管理和作业调度系统。作为一款集群作业调度管理系统，Slurm包含三大主要功能：</p><ol><li>资源分配：在特定时间段内为用户分配对计算资源的独占或非独占访问权限，以便他们可以执行作业。简单的说就是为作业提供对计算资源的授权和分配。</li><li>作业管理：它提供了在分配的节点上启动、执行和监控作业（通常是并行作业）的框架。</li><li>作业调度：通过管理待处理作业的队列来仲裁资源的争用。例如根据优先级或不同当调度策略调整资源的分配顺序。<a id="more"></a></li></ol><p>除此之外还提供了可选的插件用于计费、资源预留、多种调度策略、拓扑优化的资源选择、基于用户或账户的资源限制以及复杂的多因素作业优先级算法等功能。</p><h2 id="Slurm架构"><a href="#Slurm架构" class="headerlink" title="Slurm架构"></a>Slurm架构</h2><p><img src="http://img.chinahpc.club/2018/08/slurm-arch.gif" alt="架构图"></p><p>先来看看这张官方提供的架构图，图中描述的是单集群内Slurm核心组件的架构及依赖关系。之所以强调单集群，是因为Slurm事实上支持多集群的部署，但为了便于理解，我们这里只介绍单集群的架构，理解之后自然会对多集群部署架构有一个清晰的概念。下面我们根据该图进行详细介绍：</p><h3 id="slurmctld"><a href="#slurmctld" class="headerlink" title="slurmctld"></a>slurmctld</h3><p>Slurm有至少一个集中管理器<code>slurmctld</code>用于监控资源和作业，因为slurmctld对于整个集群管理来说至关重要，所以为了保证系统的高可用，你可以选择部署多个备用的<code>slurmctld</code>用以容灾。绝大多数功能和状态都是由slurmctld提供和管理的，安装了slurmctld的节点我们称之为主控节点，它就像是系统中的指挥官，掌控全局信息并指挥其他程序完成作业，包含以下作用：</p><ul><li>与计算节点上的slurmd进程保持通讯，负责作业下发及状态监控，slurmctld与slurmd是一对多的关系</li><li>负责集群资源的整体调度</li><li>负责集群作业队列的管理</li><li>收集资源使用信息，并交由slurmdbd记录</li><li>提供所有功能的接口，包括节点信息、作业信息、资源信息、作业控制等等</li></ul><h3 id="slurmd"><a href="#slurmd" class="headerlink" title="slurmd"></a>slurmd</h3><p>每个计算节点上都会运行slurmd进程，用以接收主控节点的指令并汇报节点及任务状态。具体作用：</p><ul><li>与主控节点交换数据</li><li>收集本机运行状态并将数据发送给主控节点</li><li>负责运行作业并在整个作业生命周期中提供管理</li></ul><h3 id="slurmdbd"><a href="#slurmdbd" class="headerlink" title="slurmdbd"></a>slurmdbd</h3><p>slurmdbd是可选的，没有它slurmd的作业系统也可以正常工作，不过实际环境中它却是必不可少的，它用于提供管理账户信息、资源使用情况、作业统计报表等等功能。主要功能：</p><ul><li>账户管理</li><li>生成报表</li><li>计费</li></ul><h3 id="命令行工具"><a href="#命令行工具" class="headerlink" title="命令行工具"></a>命令行工具</h3><p>除上述组件之外，slurm还提供了很多命令行工具，其中用户工具包括：</p><ul><li>sbatch：用于创建批处理作业</li><li>srun：用于创建交互作业</li><li>salloc: 用于获取作业资源分配</li><li>scancel：用于终止排队等候或正在运行的作业</li><li>sinfo： 用于报告系统状态</li><li>squeue： 用于报告作业状态</li><li>sacct：用于获取正在运行或已经完成的作业或作业步信息</li><li>smap和sview： 以图形方式报告系统和作业状态， 包括网络拓扑</li><li>sreport：提供报表信息</li></ul><p>管理员工具包括：</p><ul><li>scontrol： 用于监视和/或修改集群上的配置和状态信息。</li><li>sacctmgr： 数据库管理工具用于识别群集，有效用户，有效的帐户等信息。</li></ul><h3 id="插件"><a href="#插件" class="headerlink" title="插件"></a>插件</h3><p>Slurm包含了调度所需的各种功能，除基本功能之外Slurm还提供了可用于轻松支持各种基础设施的通用插件机制，这允许模块化的方式配置Slurm，下面列出部分Slurm支持的插件以及它们的功能：</p><ul><li>Accounting Storage： 主要用于存储有关作业的历史数据。当与slurmdbd一起使用时，它还可以提供基于限制的系统以及历史系统状态。</li><li>Account Gather Energy： 收集系统中每个作业或节点的能耗数据，此插件集成于Accounting Storage和Job Account Gather插件中。</li><li>Authentication of communications: 提供Slurm各种组件之间的认证机制。</li><li>Checkpoint: 提供检查机制接口</li><li>Cryptography (Digital Signature Generation)： 用于生成数字签名的机制，验证作业步被授权在特定节点上执行。这与用于验证的插件不同，因为作业步请求是从用户的srun命令而不是直接从slurmctld守护进程发出的，该插件用于生成作业步凭证及其数字签名。</li><li>Generic Resources：提供用于控制如GPU、CPU等通用资源的接口。</li><li>Job Submit： 自定义作业提交插件。</li><li>Job Accounting Gather： 收集作业步资源使用情况。</li><li>Job Completion Logging： 负责作业终止记录，所收集的数据通常是- Accounting Storage Plugin的子集。</li><li>Launchers： 控制srun命令用于启动任务的机制。</li><li>MPI： 为各种MPI实现提供不同的钩子。例如用于设置MPI特定的环境变量。</li><li>Preempt： 设定作业抢占机制。</li><li>Priority: 为作业指定优先级。</li><li>Process tracking (for signaling): 提供作业进度追踪，用于信号捕获及作业信息统计。</li><li>Scheduler: 决定作业如何调度。</li><li>Node selection: 决定资源如何分配。</li><li>Switch or interconnect: 交换机或互连接口插件。对于大多数系统（以太网或infiniband），这不是必需的。</li><li>Task Affinity: 提供绑定作业的机制，并将其各个任务分配给特定的处理器。</li><li>Network Topology：根据网络拓扑优化资源选择。用于工作分配和高级预约。</li></ul><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p><img src="http://img.chinahpc.club/2018/08/concept.gif" alt="concept"></p><p>要正确理解Slurm的工作流程，我们需要先了解一些基本概念：</p><ul><li>节点（node）：每一台安装并正确配置了slurmd进程的服务器就是一个节点。安装了slurmctld的节点虽然我们称之为主控节点，但实际上它并不是我们这里讨论的节点，这里指的节点即计算资源。</li><li>分区（partition）：分区将一组节点组合成逻辑上的一个集合，也有人称之为“队列”，实际上节点间并没有队列的概念，所以我认为“分区”是一个更合适的翻译。</li><li>作业（job）：<strong>特定时间为用户进行的一次资源申请或分配即可看作一个作业</strong>。这和我们惯性思维中的作业概念并不一致，传统意义上我们总是认为作业应该是某个运行的脚本或者程序，但事实上Slurm的作业只代表一次资源申请或分配。理解这个区别将有利于你理解Slurm中一些比较高级的用法。</li><li>作业步（job step）：Slurm中有作业步的概念，你可以理解为子作业。这允许我们在某个作业中分步骤的细分使用计算资源。</li></ul><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://slurm.schedmd.com/overview.html" target="_blank" rel="noopener">Slurm Document</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/2018/08/slurm-better-thumbnail.png&quot; alt=&quot;Cover&quot;&gt;&lt;/p&gt;
&lt;p&gt;经过前一篇文章的学习，大家应该对HPC集群调度系统有了初步的认识。接下来我们将以Slurm作为调度系统进行HPC作业相关的学习，会深入介绍Slurm的架构、配置及使用。&lt;/p&gt;
&lt;h2 id=&quot;Slurm概述&quot;&gt;&lt;a href=&quot;#Slurm概述&quot; class=&quot;headerlink&quot; title=&quot;Slurm概述&quot;&gt;&lt;/a&gt;Slurm概述&lt;/h2&gt;&lt;p&gt;Slurm是一个可工作于各种不同规模的Linux集群之上的开源、具备容错性和高度可扩展性的集群管理和作业调度系统。作为一款集群作业调度管理系统，Slurm包含三大主要功能：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;资源分配：在特定时间段内为用户分配对计算资源的独占或非独占访问权限，以便他们可以执行作业。简单的说就是为作业提供对计算资源的授权和分配。&lt;/li&gt;
&lt;li&gt;作业管理：它提供了在分配的节点上启动、执行和监控作业（通常是并行作业）的框架。&lt;/li&gt;
&lt;li&gt;作业调度：通过管理待处理作业的队列来仲裁资源的争用。例如根据优先级或不同当调度策略调整资源的分配顺序。
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC 系列文章(3):调度系统概览</title>
    <link href="http://zxh.site/2018/08/05/HPC-%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-3-Slurm%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%A7%88/"/>
    <id>http://zxh.site/2018/08/05/HPC-系列文章-3-Slurm调度系统概览/</id>
    <published>2018-08-05T01:12:33.000Z</published>
    <updated>2018-10-21T10:57:46.069Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/2018/08/schedule.png" alt="Cover"></p><h2 id="初识集群作业调度系统"><a href="#初识集群作业调度系统" class="headerlink" title="初识集群作业调度系统"></a>初识集群作业调度系统</h2><p>经过前面的简单学习，大家应该了解了HPC和集群的基本概念，集群有强大的运算力，那么如何管理集群资源的分配呢？如何管理和实现集群作业调度呢？平时我们在使用一台服务器进行一些作业时，只需要简单的登陆这台服务器执行相关指令即可。不过你是否思考过下面这些问题呢？</p><ol><li>系统中存在若干进程，操作系统会如何安排计算机资源？</li><li>是否可以为某些特定进程分配更多的资源？</li><li>如果我们使用的是多核心处理器，能否让特定的一个或多个核心运行某个程序？<a id="more"></a></li></ol><p>相信如果上学时学过计算机组成原理和操作系统的同学都能对上面几个问题有一些简单的认识。平时我们不用关心这些细节，是因为操作系统已经实现了进程的管理和调度。可见资源调度对于操作系统而言至关重要，集群是由若干台计算机组成的一个系统，它同样需要管理分配计算资源，这就是调度系统存在的意义。</p><p>但集群的调度系统和计算机操作系统的调度系统又有许多不同之处，集群的计算资源分散在系统中由网络互联，我们不可能像使用本地的计算资源一样使用它们。并且我们也不希望某个作业的资源发生竞争而影响执行效率，所以我们也不会采用轮转的方式。<strong>集群调度系统采用的是一种资源隔离和预分配的机制来调度资源。</strong>除此之外它还提供不同的资源分配策略和管理策略，我们再来思考几个问题：</p><ol><li>当系统中空闲的资源不能满足更多的作业时如何处理作业？</li><li>如何保证系统容纳尽可能多的作业？（类似背包问题）</li><li>我能否让特定的作业优先获得资源？</li><li>能否让特定的作业运行在特定配置的节点上？</li><li>在多用户环境下能否限制某些用户的资源用量？</li><li>能否统计每个用户的资源使用量？</li></ol><p>以上这些都是集群调度系统所要处理的问题，并且实际情况会更加复杂。一个优秀的调度系统应该具备多种灵活的调度策略，资源分配时要考虑多种复杂的因素来为作业分配资源，例如CPU、内存、GPU甚至节点所处的网络等，提供资源使用记录及管理，同时它还要能监控和管理节点的工作状态和资源使用情况，为集群提供尽可能高的使用效率。</p><h2 id="调度系统对比"><a href="#调度系统对比" class="headerlink" title="调度系统对比"></a>调度系统对比</h2><p>我们不会因为要做电子表格就自己做一个Excel出来，显然我们也不必因为要使用集群而自行开发这么一套系统，那市面上有哪些集群调度系统呢？打开Google搜索<code>Cluster Scheduler</code>，出来一大堆结果，这里我将我所了解的目前主流的调度系统为大家做一个简单的对比和梳理：</p><div class="note info"><p>如果你用中文搜索“集群调度系统”，得到的结果与我们所讨论的集群作业调度系统有一些区别，国内对相关领域的研究的可参考文献较少，中文搜索的结果与我们要讨论的内容有偏差。建议使用Google进行相关资料检索 </p></div><table><thead><tr><th>软件</th><th>厂商</th><th>授权许可</th><th>平台支持</th><th>最大节点数</th><th>是否收费</th></tr></thead><tbody><tr><td>OpenLava</td><td>Teraproc</td><td>GPL</td><td>Linux</td><td>数千</td><td>免费</td></tr><tr><td>PBS Pro</td><td>Altari</td><td>商业授权</td><td>Linux,Windows</td><td>50k+</td><td>付费</td></tr><tr><td>TORQUE</td><td>Adaptive Computing</td><td>商业授权</td><td>Linux, Unix</td><td>数千</td><td>付费</td></tr><tr><td>Moab Cluster Suite</td><td>Adaptive Computing</td><td>商业授权</td><td>多平台</td><td>数千</td><td>付费</td></tr><tr><td>Platform LSF</td><td>IBM</td><td>商业授权</td><td>Unix, Linux, Windows</td><td>未知</td><td>付费</td></tr><tr><td>Slurm</td><td>SchedMD</td><td>GPL</td><td>Linux,Unix</td><td>120k+</td><td>免费</td></tr></tbody></table><h3 id="术语解释"><a href="#术语解释" class="headerlink" title="术语解释"></a>术语解释</h3><p>本文中讨论的“调度系统”实际应该分成<code>Workload Manager</code>和<code>Resource Manager</code>分别讨论，但为了便于理解，我们讨论的“调度系统”即二者的合称。关于它们的区别：</p><ul><li>Resource Manager<ul><li>为集群资源提供作业队列管理</li><li>创建作业队列</li></ul></li><li>Workload Manager<ul><li>整合了一个或多个Resource Manager的调度管理器，给Resource Manager下指令</li><li>监控作业运行状态</li><li>收集和统计作业及资源使用信息</li><li>实现资源限制和策略制定等</li></ul></li></ul><h3 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h3><p>上面这些调度系统，实际上有一些有一些渊源。早期的LSF是开源的，后来成为了商用软件，<code>OpenLava</code>就是基于早期的<code>LSF</code>（Platform LSF 4.2)开发的开源软件。</p><p>然后我们说一说<code>PBS</code>，它最初是由<code>MRJ Technology Solutions</code>这家软件外包商于1991年6月开始为NASA所研发的作业调度系统，MRJ于20世纪90年代末被Veridian收购，2003年<code>Altair</code>公司通过收购Veridian获得了所有PBS技术和知识产权的权利。目前市面上有几个它的衍生产品：</p><ul><li>OpenPBS - 由MRJ于1998年发布的原始开源版本</li><li>TORQUE - 由Adaptive Computing公司（前身为Cluster Resources公司）维护的OpenPBS分支</li><li>PBS Professional（PBS Pro） - <code>Altair Engineering</code>提供的PBS版本。</li></ul><p><code>Torque</code>其实是基于<code>OpenPBS</code>开发的，早期的Torque也是开源免费软件，不过2018年6月开始TORQUE不再开源。<code>Moab</code>的前生叫<code>Maui</code>，Torque自带的scheduler功能比较弱，所以通常是需要配合Maui使用的，Torque+Maui才是我们讨论的调度系统，Maui之前是开源免费的，后来变成了商用软件<code>Moab</code>后不再免费。PBS Pro截止2016年6月之前都是商用付费软件，从2016年6月起，Altair公司宣布提供开源许可并逐步开放源码。</p><p><code>Slurm</code>前期主要由劳伦斯利弗莫尔国家实验室，SchedMD，Linux NetworX，Hewlett-Packard和Groupe Bull负责开发，它受到封闭源Quadrics RMS的启发，并且具有相似的语法。目前由社区和SchedMD公司共同维护，保持开源和免费，由SchedMD公司提供商业支持。</p><h2 id="调度系统选择"><a href="#调度系统选择" class="headerlink" title="调度系统选择"></a>调度系统选择</h2><p>上面已经分析和介绍了几款主流的调度系统，他们中的大多数如今都是商业软件，商业软件缺点是需要付费并且通常价格高昂，但它们提供一些开源免费软件所不具备的优势。例如PBS Pro背靠Altair，Altair生产了一些工业界常用的数值模拟软件，因为属于同一家公司，所以当你恰好需要的就是这些应用软件时，PBS Pro也许是不错的选择。LSF也有一些行业内的应用软件支持，并且它们都提供一些商业技术支持以及相关的完整的解决方案。所以如果你是商用、依赖特定厂商提供的应用层配套软件并且不差钱，这些商用调度系统是不错的选择。</p><p>但是综合考虑的话，我更倾向于推荐Slurm作为调度管理器。首先它是开源且免费的，我们可以学习它的源码，基于Slurm开发产品。其次从对比数据可以看出来它具备非常优秀的设计和性能，目前全球许多超算中心和超大规模的集群包括我国的天河二号等都采用Slurm作为调度系统。而且Slurm目前仍然处于活跃的开发和维护中，社区占有率和话题度都比较高，相关的学习资料也相当完善，并且官方提供了足够详细的使用文档。</p><p>后续的文章我将更加详细的介绍Slurm及其使用。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://en.wikipedia.org/wiki/Comparison_of_cluster_software" target="_blank" rel="noopener">Comparison of cluster software</a></li><li><a href="https://www.mir.wustl.edu/Portals/0/Documents/Uploads/CHPC/WashU_2_moab.pdf" target="_blank" rel="noopener">Job Scheduling with Moab Cluster Suite</a></li><li><a href="https://en.wikipedia.org/wiki/TORQUE" target="_blank" rel="noopener">TORQUE</a></li><li><a href="http://www.linuxclustersinstitute.org/workshops/archive/20th/files/11-MoabTorque/Moab-Torque.pdf" target="_blank" rel="noopener">Moab-Torque</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/2018/08/schedule.png&quot; alt=&quot;Cover&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;初识集群作业调度系统&quot;&gt;&lt;a href=&quot;#初识集群作业调度系统&quot; class=&quot;headerlink&quot; title=&quot;初识集群作业调度系统&quot;&gt;&lt;/a&gt;初识集群作业调度系统&lt;/h2&gt;&lt;p&gt;经过前面的简单学习，大家应该了解了HPC和集群的基本概念，集群有强大的运算力，那么如何管理集群资源的分配呢？如何管理和实现集群作业调度呢？平时我们在使用一台服务器进行一些作业时，只需要简单的登陆这台服务器执行相关指令即可。不过你是否思考过下面这些问题呢？&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;系统中存在若干进程，操作系统会如何安排计算机资源？&lt;/li&gt;
&lt;li&gt;是否可以为某些特定进程分配更多的资源？&lt;/li&gt;
&lt;li&gt;如果我们使用的是多核心处理器，能否让特定的一个或多个核心运行某个程序？
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC系列文章(2)：HPC技术概览</title>
    <link href="http://zxh.site/2018/07/28/HPC%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-2-%EF%BC%9AHPC%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/"/>
    <id>http://zxh.site/2018/07/28/HPC系列文章-2-：HPC技术概览/</id>
    <published>2018-07-28T08:57:13.000Z</published>
    <updated>2018-10-21T10:57:46.070Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/2018/07/AI-HPC.jpg" alt="Cover"></p><h2 id="什么是高性能运算"><a href="#什么是高性能运算" class="headerlink" title="什么是高性能运算"></a>什么是高性能运算</h2><p>在正式学习之前，我们先了解一个非常基本的问题：“究竟什么是高性能计算（HPC）？广义上来说，高性能运算就是使用高性能的计算机完成特定的计算任务。我知道这听上去是一句正确的废话，那我们将这个定义得更详细一些，就目前而言，高性能运算通常指的是通过大量的高性能的计算硬件堆叠而成的计算集群完成特定的计算任务。这里有几个关键词我们分开讨论。<a id="more"></a></p><h3 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h3><p>首先是“大量高性能计算硬件堆叠而成的计算集群”，也就是说通常不止一台。为什么不止一台呢？因为就目前的计算机科技水平而言，一台计算机的计算力是非常有限的，而提升单台机器的计算力难度是比较大的，相较之下更简单粗暴的办法就是采用更多的机器，将它们组合到一起，这听上去颇有些暴力美学的味道，集群将一组松散集成的计算机软件和/或硬件连接起来高度紧密地协作完成计算工作。</p><h3 id="并行"><a href="#并行" class="headerlink" title="并行"></a>并行</h3><p>然后是“特定的计算任务”，当机器不止一台的时候，想让它们完成同一个任务，这个任务就需要具备一些特殊性。想象一下如果将你写了一段代码或者一个脚本，假设一共100行代码，你将这段代码简单粗暴地拆分成两个50行的文件，分别拿到两台机器上各自运行，会发生什么？显然是无法工作的。那什么样的程序或者代码才能在集群上运行呢？通常该程序需要支持并行运算或者是互不干涉的进行分布式运算。集群的并行运算与我们在拥有多核处理器的单台计算机上并行运行的多线程程序有一些类似，不过集群上的并行运算更复杂一些，因为这些程序运行在不同的机器上，通常并不会直接包含其他同时运行的程序的上下文，而是通过某些特殊的通讯协议来协同作业。或者它们本就是互相不相关的独立作业，每个作业各自完成自己的工作，不必等待其他作业完成后才可以运行，互相之间也没有锁或者资源竞争。只有这样才能充分的利用集群的性能。</p><blockquote><p>并行计算（parallel computing）一般是指许多指令得以同时进行的计算模式。在同时进行的前提下，可以将计算的过程分解成小部分，之后以并发方式来加以解决。</p><p>电脑软件可以被分成数个运算步骤来运行。为了解决某个特定问题，软件采用某个算法，以一连串指令运行来完成。传统上，这些指令都被送至单一的中央处理器，以循序方式运行完成。在这种处理方式下，单一时间中，只有单一指令被运行(processor level: 比较微处理器，CISC, 和RISC，即流水线Pipeline的概念，以及后来在Pipeline基础上以提高指令处理效率为目的的硬件及软件发展，比如branch-prediction, 比如forwarding，比如在每个运算单元前的指令堆栈，汇编程序员对programm code的顺序改写)。并行运算采用了多个运算单元，同时运行，以解决问题。</p></blockquote><h3 id="高性能"><a href="#高性能" class="headerlink" title="高性能"></a>高性能</h3><p>最后一个关键词是“高性能”，这就有别于一般的分布式计算（不过它们也有很多相通之处）。不知道大家是否有了解过一些分布式计算的项目，它们召集一些志愿者贡献出自己电脑空闲的计算力，一起为一些科学计算做出贡献，这些节点对于计算节点的性能没有特别严格的限制，甚至可以不限制硬件平台，如果你愿意参与你就可以为这些计算项目贡献一点力量，这是一种更加松散的架构，包括现在有的“挖矿”项目也是类似这样的分布式计算。而高性能运算更加强调性能，包括计算单元（CPU、GPU、FPGA等）、内存、存储、网络等等一系列的硬件都应该具备很强的性能，所以高性能运算还会涉及到异构计算、并行文件系统、高速网络等等。</p><h2 id="高性能计算能做什么"><a href="#高性能计算能做什么" class="headerlink" title="高性能计算能做什么"></a>高性能计算能做什么</h2><p>高性能计算实际上服务于我们的日常生活之中，只是它的存在感很低。举个最简单的例子，天气预报。气象预测实际上应该叫做气象模拟计算，简单的说就是把卫星收集到的大量数据（你可以尽可能想象它的大），通过一些科学的计算模型进行模拟计算并推演，以此为基础来推断未来的天气。如果没有足够强大的计算力，这显然是一项无法完成的任务。提高计算力能够提高模拟计算的速度和精度，这也是为什么这些年天气预报越来越准，能预报的时间越来越长的重要原因。HPC还能应用到各种领域，例如：</p><ul><li>科学计算：例如生物科学上计算DNA序列、化学上有机化学分子计算、物理学中复杂的模型推演、数学中复杂的计算等等。</li><li>工业领域：基于现代力学理论的数值仿真技术广泛应用于航天航空、汽车、船舶、机械、建筑、电子等行业领域。</li><li>人工智能：计算能力的提升和可获得的数据的增加是深度学习的两个关键驱动因素，近些年非常热门的人工智能相关技术都依赖庞大的计算力。</li></ul><p>除此之外，计算金融、国防科技、医学成像、图像编解码、大数据分析等等各种领域都是HPC的用武之地。HPC系统最常见的用户是科学研究人员，工程师和学术机构以及一些政府机构。可以说一切需要计算力解决问题的地方，都可以使用HPC成倍的提升效率。</p><h2 id="我们要学些什么"><a href="#我们要学些什么" class="headerlink" title="我们要学些什么"></a>我们要学些什么</h2><p>集群中的计算力需要有一个合理规范的调度系统，负责分配作业调度,提高集群资源使用率。所以我们需要了解学习HPC集群调度系统。目前常见的HPC集群作业调度系统有Altair公司的<code>PBS</code>、开源的<code>Slurm</code>和IBM公司的<code>LSF</code>，这个系列中主要围绕<code>Slurm</code>进行介绍。</p><p>集群中运行的任务具备一些特殊性，要想发挥出集群的优势，作业需要支持并行运算，或者在集群上运行不相互依赖的分布式作业，所以会简单介绍并行计算以及一些分布式计算相关的知识和技巧。</p><p>大量的设备需要有一个高效可靠的监控和管理系统，所以我也会了解一些分布式监控系统以及一些常见的集群运维手段和工具。</p><p>目前云服务和虚拟化非常火热，并且也可以说是未来的一种技术趋势，在了解和掌握基本的高性能运算基础之上，我还会额外介绍一些虚拟化和云相关的知识，探讨一下高性能计算和云计算如何结合等等。</p><p>概括一下大概有以下内容：</p><ul><li>集群调度系统</li><li>并行计算及部分应用</li><li>集群监控及管理</li><li>云及容器技术</li></ul><p>我个人能力和水平有限，有的东西也是在不断学习的过程中，有任何讲的不对或者不合理的地方，欢迎大家在评论区进行讨论，共同学习进步。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/2018/07/AI-HPC.jpg&quot; alt=&quot;Cover&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;什么是高性能运算&quot;&gt;&lt;a href=&quot;#什么是高性能运算&quot; class=&quot;headerlink&quot; title=&quot;什么是高性能运算&quot;&gt;&lt;/a&gt;什么是高性能运算&lt;/h2&gt;&lt;p&gt;在正式学习之前，我们先了解一个非常基本的问题：“究竟什么是高性能计算（HPC）？广义上来说，高性能运算就是使用高性能的计算机完成特定的计算任务。我知道这听上去是一句正确的废话，那我们将这个定义得更详细一些，就目前而言，高性能运算通常指的是通过大量的高性能的计算硬件堆叠而成的计算集群完成特定的计算任务。这里有几个关键词我们分开讨论。
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC系列文章(1)：开篇</title>
    <link href="http://zxh.site/2018/07/22/HPC%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-1-%EF%BC%9A%E5%BC%80%E7%AF%87/"/>
    <id>http://zxh.site/2018/07/22/HPC系列文章-1-：开篇/</id>
    <published>2018-07-22T05:47:50.000Z</published>
    <updated>2018-10-21T10:57:45.882Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/2018/07/AI-HPC.jpg" alt="Cover"></p><h1 id="写在最前"><a href="#写在最前" class="headerlink" title="写在最前"></a>写在最前</h1><p>HPC系列文章是一系列关于高性能计算的技术文章，计划每周更新至少一篇高性能计算相关的文章。为什么要写这些文章呢？首要原因是我个人从事相关的研发工作，我一直认为<code>Learning by teaching</code>是最佳的学习和验证学习效果的方式，所以我想将个人了解的相关知识在这里进行梳理。另外一方面是高性能运算实际上服务于我们每个人的日常生活中，被许多热门技术和行业所依赖，但鲜有人深入了解，真正从事这个行业的人并不像Web开发、深度学习等多，针对高性能运算的技术文章也相对较少，我想将这些文章整理成一个系列供感兴趣的朋友学习了解。<a id="more"></a></p><p>高性能运算涉及的技术领域非常广泛，从高性能硬件基础设施、网络、存储、复杂的调度系统、异构计算、并行计算、繁多的应用软件、虚拟化、云计算等等都会涵盖。可以说每个知识面单独拿出来讲都非常复杂，我自然也不是每个领域都深入了解和涉及，我个人主要从事的是调度系统及虚拟化技术的研究，所以该系列文章也主要包含这些内容。</p><p>高性能运算能用在很多领域，包括科学计算、模拟运算、编解码、深度学习等等都可以通过高性能运算成倍的提速。近几年人工智能产业越来越热，也为高性能计算带来新的生机，高性能计算人才稀缺，整体来说高性能运算是一个非常有价值且正在逐步发展的研究方向。但是正如前面所说，高性能运算涵盖的知识非常多，门槛比较高，我希望这个系列文章能为大家打开这扇门，降低学习的曲线，系统地学习和掌握高性能运算相关的技术。</p><p>我在学习高性能计算相关技术的过程，因为每个领域都有很多可以学习的东西，持续学习的过程可以说是惊喜不断。我也希望能通过这个系列文章为大家带来一些技术上的突破，和大家一起共同进步。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/2018/07/AI-HPC.jpg&quot; alt=&quot;Cover&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;写在最前&quot;&gt;&lt;a href=&quot;#写在最前&quot; class=&quot;headerlink&quot; title=&quot;写在最前&quot;&gt;&lt;/a&gt;写在最前&lt;/h1&gt;&lt;p&gt;HPC系列文章是一系列关于高性能计算的技术文章，计划每周更新至少一篇高性能计算相关的文章。为什么要写这些文章呢？首要原因是我个人从事相关的研发工作，我一直认为&lt;code&gt;Learning by teaching&lt;/code&gt;是最佳的学习和验证学习效果的方式，所以我想将个人了解的相关知识在这里进行梳理。另外一方面是高性能运算实际上服务于我们每个人的日常生活中，被许多热门技术和行业所依赖，但鲜有人深入了解，真正从事这个行业的人并不像Web开发、深度学习等多，针对高性能运算的技术文章也相对较少，我想将这些文章整理成一个系列供感兴趣的朋友学习了解。
    
    </summary>
    
      <category term="高性能计算" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/HPC/"/>
    
    
      <category term="高性能计算" scheme="http://zxh.site/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>在Deepin上安装TensorFlow</title>
    <link href="http://zxh.site/2018/07/15/%E5%9C%A8Deepin%E4%B8%8A%E5%AE%89%E8%A3%85TensorFlow/"/>
    <id>http://zxh.site/2018/07/15/在Deepin上安装TensorFlow/</id>
    <published>2018-07-15T11:51:33.000Z</published>
    <updated>2018-10-21T10:57:46.070Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/2018/07/tensorflowdeepin.png" alt="NVIDIA-SMI"><br>Tensorflow是目前热门的深度学习框架，官方提供了较为详细的安装文档，不过是针对主流操作系统的，并不包含Deepin这种国产发行版，所以在安装过程中遇到一些问题，这里简单记录一下安装流程。<a id="more"></a></p><h2 id="系统配置"><a href="#系统配置" class="headerlink" title="系统配置"></a>系统配置</h2><ul><li>Deepin 15.6桌面版</li><li>NVIDIA 1060GTX</li><li>显卡驱动版本：387.34-3deepin</li><li>CUDA版本：8.0</li><li>Tensorflow版本：Tensorflow-GPU(1.4.1)</li><li>cuDNN版本：6.0</li></ul><blockquote><p>注意：请务必注意上述版本，Tensorflow安装最大的坑就在于官方文档总是提供最新的版本，而实际上最新的版本不一定适应于你当前的硬件！</p></blockquote><h2 id="安装NVIDIA显卡驱动"><a href="#安装NVIDIA显卡驱动" class="headerlink" title="安装NVIDIA显卡驱动"></a>安装NVIDIA显卡驱动</h2><p>Deepin默认使用的是开源驱动，该驱动兼容性较好但是性能比NVIDIA原生闭源驱动要差。所以我们要先切换一下驱动。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install nvidia-driver <span class="comment">#安装驱动包</span></span><br></pre></td></tr></table></figure><p>安装完成后重启系统，在终端中执行nvidia-smi将会看到类似输出：</p><p><img src="http://img.chinahpc.club/2018/07/nvidia-smi.png" alt="NVIDIA-SMI"></p><blockquote><p>注意: NVIDIA官网目前提供的最新驱动版本是390，但我安装之后导致系统黑屏驱动无法工作，如无必要不建议安装NVIDIA官网提供的最新版本。</p></blockquote><h2 id="安装CUDA"><a href="#安装CUDA" class="headerlink" title="安装CUDA"></a>安装CUDA</h2><p>因为Deepin和Ubuntu实际上都是基于Debian的，所以我们可以看Tensorflow中Ubuntu的文档进行安装，根据Tensorflow官网提供的链接，NVIDIA将建议安装CUDA9.2，不过这里我们直接用源内提供的8.0版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install nvidia-cuda-dev nvidia-cuda-toolkit nvidia-nsight nvidia-visual-profiler</span><br></pre></td></tr></table></figure><h2 id="安装cuDNN"><a href="#安装cuDNN" class="headerlink" title="安装cuDNN"></a>安装cuDNN</h2><p>安装cuDNN需要进行简单的注册并填写一个简单的调查问卷，注册完成之后即可下载。</p><p>cuDNN也要分很多版本（<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">版本列表</a>），这里我们选择<code>cuDNN v6.0 (April 27, 2017), for CUDA 8.0</code>这个版本。直接下载压缩包(<a href="https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v6/prod/8.0_20170307/cudnn-8.0-linux-x64-v6.0-tgz" target="_blank" rel="noopener">cuDNN v6.0 Library for Linux</a>)解压就好：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/.<span class="built_in">local</span> <span class="comment"># 如果没有这个文件夹请手动新建</span></span><br><span class="line">tar zxvf ~/Software/cudnn-8.0-linux-x64-v6.0.tgz</span><br></pre></td></tr></table></figure><p>配置LD_LIBRARY_PATH：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建议直接将这个写到bashrc或者zshrc中</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=~/.<span class="built_in">local</span>/cuda/lib64</span><br></pre></td></tr></table></figure><h2 id="安装Tensorflow"><a href="#安装Tensorflow" class="headerlink" title="安装Tensorflow"></a>安装Tensorflow</h2><p>首先安装python和pip，然后执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorflow-gpu==1.4.1</span><br></pre></td></tr></table></figure><p>验证一下安装情况，在python交互模式中输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow</span><br></pre></td></tr></table></figure><p>如果没有任何报错就说明安装好了，跑一个官网最简单的<a href="https://www.tensorflow.org/get_started/get_started_for_beginners?hl=zh-cn" target="_blank" rel="noopener">对鸢尾花进行分类</a>的demo试试吧，你将看到类似这样的输出：</p><p><img src="http://img.chinahpc.club/2018/07/demo.png" alt="NVIDIA-SMI"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/2018/07/tensorflowdeepin.png&quot; alt=&quot;NVIDIA-SMI&quot;&gt;&lt;br&gt;Tensorflow是目前热门的深度学习框架，官方提供了较为详细的安装文档，不过是针对主流操作系统的，并不包含Deepin这种国产发行版，所以在安装过程中遇到一些问题，这里简单记录一下安装流程。
    
    </summary>
    
      <category term="Linux" scheme="http://zxh.site/categories/Linux/"/>
    
      <category term="深度学习" scheme="http://zxh.site/categories/Linux/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Linux" scheme="http://zxh.site/tags/Linux/"/>
    
      <category term="深度学习" scheme="http://zxh.site/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Deepin——优秀的国产Linux发行版</title>
    <link href="http://zxh.site/2018/07/15/Deepin-%E4%BC%98%E7%A7%80%E7%9A%84%E5%9B%BD%E4%BA%A7Linux%E5%8F%91%E8%A1%8C%E7%89%88/"/>
    <id>http://zxh.site/2018/07/15/Deepin-优秀的国产Linux发行版/</id>
    <published>2018-07-15T04:01:27.000Z</published>
    <updated>2018-10-21T10:57:45.884Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/2018/07/desktop.png" alt="Desktop"></p><p>最近想接触一下深度学习相关的东西，需要用到高性能的GPU做支持，家里吃灰大半年的PC终于有机会重新发光发热了。之前购买这台PC主要是为了玩一些大型的单机游戏，配置还算凑合，有1060GTX的显卡+双固态硬盘，装的当然是Window的操作系统。不过Windows真的不太适合用来做这方面的开发，所以就打算在第二块移动硬盘上安装Linux的操作系统。</p><p>我平时做开发都是在Linux下，所以对Linux还算比较熟悉，但是平时几乎从来不用桌面环境，不过这次需要比较直观的图像输出，就选了Deepin这款基于Debian的国产桌面操作系统。<a id="more"></a>事实上我在15年就在一台笔记本上安装体验了Deepin，那时候体验就已经不错了，不过系统中存在一些小bug，稳定性一般，加上当时开发的东西主要在CentOS上运行，就没有再深入体验。这两天我花了一些时间搭建开发环境，顺便把它弄得更顺手，这个过程中Deepin总体的使用体验远超我的预期。两年的时间，Deepin已经成熟。</p><h2 id="外观"><a href="#外观" class="headerlink" title="外观"></a>外观</h2><p>在这个颜值即正义的时代，检验一个桌面操作系统是否优秀的第一关就是外观，Deepin吸收借鉴了macOS和Windows的很多优秀设计，又将它们各自的槽点进行了改进，形成了一套相当好用的桌面环境。例如多工作区、Dock栏以及多任务视图借鉴了macOS，而文件管理器及菜单中的很多操作更接近于Windows。</p><p>有一些人吐槽说这是抄袭，我不太认可这样的观点，我认为原封不动的照搬才是抄袭，而懂得取舍和改进应该算作创新。</p><h3 id="菜单"><a href="#菜单" class="headerlink" title="菜单"></a>菜单</h3><p>Deepin的菜单有三种不同的展示方式，缩小可以变成类似Windows的菜单，放大后类似macOS将所有应用平铺在屏幕中，此外还可以按类别来展示。我个人更喜欢Windows那样的，按下菜单键后可以直接输入应用名打开程序，比较高效。</p><p><img src="http://img.chinahpc.club/2018/07/menu-win.png" alt="Win"></p><p><img src="http://img.chinahpc.club/2018/07/menu.png" alt="Menu"></p><h3 id="侧边栏"><a href="#侧边栏" class="headerlink" title="侧边栏"></a>侧边栏</h3><p>如今macOS和Windows10都有侧边栏，一个好用的侧边栏会让系统体验更好。Deepin也弄了一个侧边栏，设计还算简洁，不过目前功能比较简单，类似一个微缩的控制面板，而且貌似暂不支持小插件。上半部分是一个可以切换的区域，通知被折叠到了第三页，天气放在第二页。使用体验上不如macOS从上往下滚动舒服，但是比windows的要强一点点，这部分还有较大的优化空间。</p><p><img src="http://img.chinahpc.club/2018/07/desktop.png" alt="桌面"></p><h3 id="多工作区"><a href="#多工作区" class="headerlink" title="多工作区"></a>多工作区</h3><p>熟悉macOS的用户应该都非常喜欢macOS的多桌面设计，你可以将不同的软件放到不同的桌面并且可以通过触控板上的手势操作快速切换，macOS可以说把多桌面体验做到了机制。Deepin在这一点上与macOS非常相似，整体使用下来感觉只差一个带手势操作的触控板了。</p><p><img src="http://img.chinahpc.club/2018/07/multi-tasks.png" alt="多任务"></p><p>按下Super+s（Super即常见键盘的Win键），就可以快速呼出所有的应用，你可以轻易的将它们排列和切换。</p><p><img src="http://img.chinahpc.club/2018/07/multi-workspace.png" alt="多工作区"></p><p> 按下Super+方向键，可以快速在相邻的工作区切换，操作非常方便。与macOS最大的不同是应用最大化之后，macOS中该最大化的应用会独占整个工作区，而Deepin支持多个最大化的应用在同一个工作区。这一点上我个人更喜欢macOS的设计，软件最大化后连菜单栏都会收缩起来，让你可以获得沉浸式的工作体验。</p><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>Deepin的外观设计我个人还是很满意的，不过真正让我觉得惊喜的是它的一些特点和以代码开发人员的思维所做出的一些高效设计。</p><h3 id="兼容常用Windows软件"><a href="#兼容常用Windows软件" class="headerlink" title="兼容常用Windows软件"></a>兼容常用Windows软件</h3><p>Linux在桌面市场上之所以不够流行，其主要原因是桌面软件对Linux支持较少，而Deepin通过精心改造的wine让一些常用的Windows软件可以在系统中运行。例如我们常用的微信桌面版、QQ、网易云音乐、迅雷等等，都可以从Deepin的应用商店里直接安装使用，这解决了很多用户的一大痛点。</p><p><img src="http://img.chinahpc.club/2018/07/store.png" alt="多工作区"></p><h3 id="为普通人做的设计"><a href="#为普通人做的设计" class="headerlink" title="为普通人做的设计"></a>为普通人做的设计</h3><p>内置常用软件，提供常用软件的应用商店，内置中文输入法极大降低了普通用户上手Linux的使用成本。国人大多数都是更习惯Windows的操作方式，例如打开“计算机”应该看到磁盘分区，<code>Ctrl+X/C/V</code>分别对应剪切、复制和粘贴。坦白说尽管我欣赏macOS的很多设计，但是在文件管理器上macOS的体验真的算不上优秀，而类windows的操作效率会更高一些。</p><h3 id="为程序员做的设计"><a href="#为程序员做的设计" class="headerlink" title="为程序员做的设计"></a>为程序员做的设计</h3><p><img src="http://img.chinahpc.club/2018/07/terminal.png" alt="终端"></p><p>除了照顾普通大众，Deepin针对程序员也做了一些优化，例如尽可能多的快捷键，其中包括<code>Alt+F2</code>快速启动终端、内置快速截图、菜单中就可以设置软件安装源、内置优秀的终端软件等等，这些就算对有经验的Linux用户来说也是一种便利。</p><p><img src="http://img.chinahpc.club/2018/07/source.png" alt="软件源"></p><h2 id="配置技巧"><a href="#配置技巧" class="headerlink" title="配置技巧"></a>配置技巧</h2><p>我花了一整天的时间将Deepin打造得更符合我的使用习惯，这里简单分享一下一些优化和遇到的坑</p><h3 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h3><p>推荐直接安装应用商店中的shadowsocks-qt,然后配合genpac实现智能代理，再安装上glider实现HTTP代理，这几款软件的安装和使用都非常简单，这里我分享一下我的glider服务配置：<a href="https://gist.github.com/ansiz/5ed26da1dbad594fb09ceca3fa927eb7" target="_blank" rel="noopener">GitHubGist</a></p><h3 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h3><p>系统默认的字体不算好看，我目前使用的是苹方字体，等宽字体推荐Noto Mono，终端字体我用的<a href="https://github.com/tonsky/FiraCode" target="_blank" rel="noopener">Fira Code Retina</a>，系统对Emoji的支持也比较差，如果你用chrome作为浏览器并且希望能看到彩色的emoji的话，我建议不要各种折腾emoji字体了，直接安装chromoji这个插件。一定要安装emoji字体的话，推荐<a href="https://github.com/emojione/emojione" target="_blank" rel="noopener">emojione</a></p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><h4 id="窗口特效开启失败"><a href="#窗口特效开启失败" class="headerlink" title="窗口特效开启失败"></a>窗口特效开启失败</h4><p>开启窗口特效后会减少画面的撕裂感，显示更为流畅，但需要更多的系统资源。安装完闭源显卡驱动后开启窗口特效失败，需要手动修改<code>~/.config/deepin/deepin-wm-switcher/config.json</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;allow_switch&quot;:true,&quot;last_wm&quot;:&quot;deepin-wm&quot;&#125;</span><br></pre></td></tr></table></figure><h4 id="终端配置选中即复制"><a href="#终端配置选中即复制" class="headerlink" title="终端配置选中即复制"></a>终端配置选中即复制</h4><p>终端选中即复制默认未打开，需要手动配置copy_on_select：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.config/deepin/deepin-terminal/config.conf</span><br><span class="line"></span><br><span class="line">copy_on_select=<span class="literal">true</span></span><br></pre></td></tr></table></figure><p>Tips：按鼠标中键就是粘贴</p><h4 id="Vim禁用鼠标"><a href="#Vim禁用鼠标" class="headerlink" title="Vim禁用鼠标"></a>Vim禁用鼠标</h4><p><code>Vim</code>会自动进入鼠标模式，影响内容复制，请手动配置<code>~/.vimrc</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mouse=</span><br></pre></td></tr></table></figure><h2 id="目前发现的问题"><a href="#目前发现的问题" class="headerlink" title="目前发现的问题"></a>目前发现的问题</h2><ul><li>侧边栏弹出的逻辑不清晰，响应不及时</li><li>拖动操作bug，无论是拖动终端中的Tab还是拖动文件夹中的文件重新排布顺序都无法实现</li><li>字体支持不完整，Emoji支持较差</li><li>暂不支持NVIDIA最新驱动，安装最新的390驱动会导致系统无法启动，目前可以使用387版</li><li>偶尔会出现假死机的情况，现象是鼠标点击无效，键盘操作正常，<a href="https://wiki.deepin.org/index.php?title=%E7%B3%BB%E7%BB%9F%E6%AD%BB%E6%9C%BA" target="_blank" rel="noopener">官方文档</a>也有提到该问题，希望能尽快改善减少。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总的来说Deepin是一款非常优秀的桌面Linux系统，经过多年的打磨，各方面都已经日趋完善，是一款能够满足办公娱乐及开发的桌面系统。</p><p>我还在上小学的时候连音像店都会卖深度快速装机光盘，那是深度起家的黑历史，难以想象当初靠盗版起家的深度自主研发的操作系统能做到这个水准。吸收了很多macOS和Windows的优秀设计，又摒弃了它们的一些槽点，很多细节都是按照开发人员的思维设计的，一些细节上甚至会给人惊喜。令人担心的是深度的CTO兼创始人上个月离职了，希望不要因此而停止这个国产优秀系统的开发，期待更加优秀的Deepin。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/2018/07/desktop.png&quot; alt=&quot;Desktop&quot;&gt;&lt;/p&gt;
&lt;p&gt;最近想接触一下深度学习相关的东西，需要用到高性能的GPU做支持，家里吃灰大半年的PC终于有机会重新发光发热了。之前购买这台PC主要是为了玩一些大型的单机游戏，配置还算凑合，有1060GTX的显卡+双固态硬盘，装的当然是Window的操作系统。不过Windows真的不太适合用来做这方面的开发，所以就打算在第二块移动硬盘上安装Linux的操作系统。&lt;/p&gt;
&lt;p&gt;我平时做开发都是在Linux下，所以对Linux还算比较熟悉，但是平时几乎从来不用桌面环境，不过这次需要比较直观的图像输出，就选了Deepin这款基于Debian的国产桌面操作系统。
    
    </summary>
    
      <category term="Linux" scheme="http://zxh.site/categories/Linux/"/>
    
      <category term="评测" scheme="http://zxh.site/categories/Linux/%E8%AF%84%E6%B5%8B/"/>
    
    
      <category term="Linux" scheme="http://zxh.site/tags/Linux/"/>
    
      <category term="评测" scheme="http://zxh.site/tags/%E8%AF%84%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>安全的SOCKS5协议</title>
    <link href="http://zxh.site/2018/07/08/%E5%AE%89%E5%85%A8%E7%9A%84socks5%E5%8D%8F%E8%AE%AE/"/>
    <id>http://zxh.site/2018/07/08/安全的socks5协议/</id>
    <published>2018-07-08T07:27:48.000Z</published>
    <updated>2018-10-21T10:57:45.978Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/2018/07/malicious.png" alt="malicious"></p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在<code>HTTP</code>/<code>HTTPS</code>的世界里，<code>TCP/IP</code>数据包的源和目标是公开的。恶意的第三方可以干扰网络连接，将污染过的数据包发送给请求源，从而实现阻止或中断连接。<a id="more"></a></p><p>目前已知的攻击手段包括：</p><ul><li>IP封锁 - 对指定IP地址的任何连接尝试都会被恶意的第三方阻止。</li><li>DNS欺骗 - 返回错误的IP地址。</li><li>包过滤 - 当包中被检测到特定的词句时该数据包将会被阻止或丢弃。</li><li>TCP RST - 对在黑名单中的目的地址进行访问时会发送<code>RST</code>包，TCP连接将会立即中断。</li><li>深度学习 - 利用集群深度学习，恶意方可以快速找到可疑的IP地址。</li></ul><p>在这样的封锁下，想要在自由的网络中呼吸新鲜空气听起来像是在做梦。但是极客们显然不愿意因此丢失任何一个数据包。</p><p>解决方案是利用强加密算法的<a href="https://zh.wikipedia.org/wiki/SOCKS" target="_blank" rel="noopener">SOCKS5</a>协议。</p><h2 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h2><ul><li>穿透防火墙封锁</li><li>隐藏所有数据包数据，包括源，目标，载荷等</li></ul><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><p>下面介绍常见的安全SOCKS5代理的开发技术及其工作原理</p><h3 id="工作模式介绍"><a href="#工作模式介绍" class="headerlink" title="工作模式介绍"></a>工作模式介绍</h3><h4 id="自由网络"><a href="#自由网络" class="headerlink" title="自由网络"></a>自由网络</h4><p>在没有恶意攻击者的情况下，客户端与服务器端可以正常通讯。</p><p><img src="http://img.chinahpc.club/2018/07/free-client-server.gif" alt="free"></p><h4 id="被封锁的网络"><a href="#被封锁的网络" class="headerlink" title="被封锁的网络"></a>被封锁的网络</h4><p>恶意攻击者可以在客户端和服务器之间部署类似防火墙的系统，这样任何请求都不会到达服务器。因此，最终用户无法从服务器获取任何数据。</p><p><img src="http://img.chinahpc.club/2018/07/blocking.gif" alt="blocked"></p><h4 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h4><p>一种合理的方法是在防火墙之外设置代理服务器。所有流量都需要先通过代理服务器，然后由代理服务器中继。目前代理技术包括HTTP代理，Socks服务，VPN服务，SSH隧道等。SOCKS5也是其中一种代理协议。</p><p><img src="http://img.chinahpc.club/2018/07/proxying.gif" alt="proxy"></p><p>举个例子，当请求通过SSH隧道传输时，防火墙无法识别流量，因为数据已加密。但问题是，在创建隧道时进行SSH握手时，很可能会发现代理服务器被用作代理。因此防火墙可以阻止SSH隧道创建步骤的网络连接。</p><h4 id="安全代理"><a href="#安全代理" class="headerlink" title="安全代理"></a>安全代理</h4><p>安全的socks代理不应向防火墙公开以下信息：</p><ul><li>任何表明它被用作代理的特征</li><li>任何真实传输的数据</li></ul><p><img src="http://img.chinahpc.club/2018/07/secure-proxy.gif" alt="secure-proxy"></p><p>要解决上述模型中存在的问题，<code>SOCKS5</code>需要进行改进。我们可以把<code>SOCKS5</code>拆分成两部分，<code>socks5-local</code>和<code>socks5-remote</code></p><p>上图中SOCKS5的工作原理主要可以概括为以下几步：</p><ul><li>客户端通过<code>SOCKS5</code>协议向本地代理发送请求。</li><li>本地代理通过<code>HTTP</code>协议发送加密后的请求数据。</li><li>因为HTTP协议没有明显的特征，并且远程代理服务器尚未被识别为代理，因此请求可以穿透防火墙。</li><li>远程代理服务器解密数据后中继到真实服务。</li><li>真实服务将响应返回到远程代理服务器，并通过相同的路径传播回客户端。</li></ul><h3 id="使用SOCKS5作为本地代理协议"><a href="#使用SOCKS5作为本地代理协议" class="headerlink" title="使用SOCKS5作为本地代理协议"></a>使用SOCKS5作为本地代理协议</h3><p><code>SOCKS5</code>在客户端有广泛的支持。我们很容易得到通过SOCKS5从客户端传输数据到本地代理服务器的好处。 SOCKS5在第5层上执行数据传输。</p><p>下面的curl命令演示了客户端能够使用SOCKS5连接到SOCKS5代理服务器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl --socks5 127.0.0.1:1080 https://google.com</span><br></pre></td></tr></table></figure><h3 id="为什么使用HTTP作为传输协议"><a href="#为什么使用HTTP作为传输协议" class="headerlink" title="为什么使用HTTP作为传输协议"></a>为什么使用HTTP作为传输协议</h3><p>HTTP可能是整个互联网上最常见的流量类型。选择HTTP作为传输协议以突破防火墙使得数据包和服务器不太可能被识别用于代理使用。</p><p>HTTPS不适合防火墙突破。因为HTTPS的设计目的是：</p><ul><li>避免恶意方改变内容</li><li>避免恶意方伪装成目标服务器</li><li>数据加密</li></ul><p>而代理软件的目的应满足以下要求：</p><ul><li>避免目标服务器被识别</li><li>避免代理服务器被识别为代理</li><li>数据加密</li></ul><p>基于以上原因，HTTP是突破防火墙比HTTPS或任何其他协议更好的选择。</p><h3 id="加密算法"><a href="#加密算法" class="headerlink" title="加密算法"></a>加密算法</h3><p>密码算法对数据进行加密和解密，这样任何人都无法从加密流中读取数据，除了LocalProxy和RemoteProxy。由于<code>AEAD</code>密码同时提供机密性，完整性和真实性，建议选择下面列出的AEAD系列之一作为在本地代理和远程代理中使用的密码算法：</p><ul><li>chacha20-ietf-poly1305</li><li>aes-256-gcm</li><li>aes-128-gcm</li></ul><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>Shadowsocks是一款出色的安全socks5代理解决方案。查看其<a href="https://shadowsocks.org/en/index.html" target="_blank" rel="noopener">官方网站</a>了解更多信息。</p><h2 id="挑战和风险"><a href="#挑战和风险" class="headerlink" title="挑战和风险"></a>挑战和风险</h2><h3 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h3><ul><li>恶意方可以在客户端安装干扰软件，以阻止本地软件和LocalProxy之间的连接。<ul><li>这可以通过强制安装来实现</li><li>它也可以通过嵌入广泛安装的软件来实现</li></ul></li><li>恶意方可能会强制该人卸载LocalProxy</li></ul><h3 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a>服务端</h3><p>必须将服务器端部署到没有防火墙阻止的环境中</p><ul><li>恶意方可以强制云提供商或潜入云服务器做以下事情：<ul><li>窃取服务器日志文件</li><li>杀死ServerProxy进程</li></ul></li><li>恶意方也可以是云提供商</li></ul><h3 id="不安全的加密算法"><a href="#不安全的加密算法" class="headerlink" title="不安全的加密算法"></a>不安全的加密算法</h3><p>使用何种加密算法取决于用户，如果最终用户选择不安全的密码算法，可能会增加服务器被识别为代理服务器的风险。提供代理服务的服务器将被封锁。</p><h3 id="IP白名单限制"><a href="#IP白名单限制" class="headerlink" title="IP白名单限制"></a>IP白名单限制</h3><p>如果最终用户只能连接到选定范围的IP列表（即IP白名单），则用户无法使用SOCKS5服务器，因为SOCKS5服务器IP不太可能包含在IP白名单中。</p><h3 id="进一步的风险"><a href="#进一步的风险" class="headerlink" title="进一步的风险"></a>进一步的风险</h3><p>当最终用户泄露密码密钥和流量历史记录时，恶意方可以解密过去的所有流量。</p><h3 id="执法监管"><a href="#执法监管" class="headerlink" title="执法监管"></a>执法监管</h3><p>即使使用安全的SOCKS5代理，也要注意不要泄露任何个人信息。卸载掉那些不可信任的安全证书和软件，据说<code>Shadowsocks</code>的发明者因为通过QQ与朋友聊天最后被“喝茶”。 :)</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过秘密部署安全的SOCKS5代理服务器并选择强密码算法，人们可以突破严重的网络封锁。 <code>Shadowsocks</code>可能是您的首选。</p><hr><p>本文翻译自<a href="https://enqueuezero.com/secure-socks5-proxy.html" target="_blank" rel="noopener">secure-socks5-proxy</a>，包含少量修改。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://enqueuezero.com/secure-socks5-proxy.html" target="_blank" rel="noopener">原文</a></li><li><a href="https://zhuanlan.zhihu.com/p/28566058" target="_blank" rel="noopener">什么是AEAD加密</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/2018/07/malicious.png&quot; alt=&quot;malicious&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在&lt;code&gt;HTTP&lt;/code&gt;/&lt;code&gt;HTTPS&lt;/code&gt;的世界里，&lt;code&gt;TCP/IP&lt;/code&gt;数据包的源和目标是公开的。恶意的第三方可以干扰网络连接，将污染过的数据包发送给请求源，从而实现阻止或中断连接。
    
    </summary>
    
      <category term="网络" scheme="http://zxh.site/categories/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="译文" scheme="http://zxh.site/categories/%E7%BD%91%E7%BB%9C/%E8%AF%91%E6%96%87/"/>
    
    
      <category term="网络" scheme="http://zxh.site/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="译文" scheme="http://zxh.site/tags/%E8%AF%91%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>HPC环境下使用Docker</title>
    <link href="http://zxh.site/2018/07/01/docker-on-cluster/"/>
    <id>http://zxh.site/2018/07/01/docker-on-cluster/</id>
    <published>2018-07-01T04:29:09.000Z</published>
    <updated>2018-10-21T10:57:46.070Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img.chinahpc.club/2018/07/docker-banner2.jpg" alt="docker"><br>HPC应用总类繁多，各种软件可能运行在不同的系统平台之上，容器技术正是解决这类问题的绝佳手段。我在<a href="http://blog.zxh.site/2018/05/15/Singularity/" target="_blank" rel="noopener"><em>Singularity——HPC环境的绝佳容器解决方案</em></a>一文中介绍了<code>Singularity</code>这个HPC下非常适合的容器技术。但是Singularity也有一些不尽人意的地方，例如缺少虚拟化网络、热度不如<code>Docker</code>高等。那如果想在一个高性能集群中使用Docker又会遇到什么问题呢？有没有什么好的解决方案？<a id="more"></a></p><h2 id="需要解决的问题"><a href="#需要解决的问题" class="headerlink" title="需要解决的问题"></a>需要解决的问题</h2><h3 id="非root用户运行Docker"><a href="#非root用户运行Docker" class="headerlink" title="非root用户运行Docker"></a>非root用户运行Docker</h3><p>在许多系统中你尝试以普通用户身份运行Docker指令，通常会得到如下提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.37/containers/json: dial unix /var/run/docker.sock: connect: permission denied</span><br></pre></td></tr></table></figure><p>Docker默认不支持普通用户直接运行Docker命令，这里有一篇不错的文章说明了为什么我们不允许非root用户直接运行Docker命令：<a href="http://dockone.io/article/589" target="_blank" rel="noopener"><em>为什么我们不允许非root用户在CentOS、Fedora和RHEL上直接运行Docker命令</em></a>,文章中提到了两个解决方案：</p><ul><li>将非root用户加入<code>docker</code>用户组</li><li>设置<code>sudo</code>规则</li></ul><p>但是这样会给管理员带来一些额外的工作量，不方便与类似<code>Slurm</code>之类的软件配合使用。</p><h3 id="资源限制"><a href="#资源限制" class="headerlink" title="资源限制"></a>资源限制</h3><p>Docker运行指令实际上会由Docker daemon去执行，所以当你将启动容器的指令包装到一个脚本中再通过调度系统去运行时，集群资源限制会失去作用。原因是它们是由完全不相关的进程去负责启动的，<code>cgroup</code>限制自然会失效。这样一来虽然环境是隔离了，但是失去了资源限制，会影响集群资源分配策略。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>为了解决上述问题，我查询了很多资料，有一篇IEEE论文中提出了一个思路： <a href="https://ieeexplore.ieee.org/document/7923813/" target="_blank" rel="noopener"><em>Enabling Docker Containers for High-Performance and Many-Task Computing</em></a>，我用<code>Golang</code>按照作者的思路开发了一个名为<code>go-socker</code>的工具（GitHub仓库：<a href="https://github.com/ansiz/go-socker" target="_blank" rel="noopener">go-socker</a>）。针对上面的两个问题，<code>go-socker</code>给出了解决方案。<code>go-socker</code>本质上就是封装了一些基本的Docker指令，然后我们将编译后的可执行程序owner设置为root，再为其加上特殊的<code>s</code>权限位，这样普通用户执行的命令就会在程序内被提升到root。<br>关于这个特殊的<code>s</code>权限位介绍： <a href="http://cn.linux.vbird.org/linux_basic/0220filemanager_4.php#suid" target="_blank" rel="noopener">鸟哥的Linux私房菜——第七章、Linux文件与目录管理</a></p><blockquote><p>当 s 这个标志出现在文件拥有者的 x 权限上时，例如刚刚提到的 /usr/bin/passwd 这个文件的权限状态：『-rwsr-xr-x』，此时就被称为 Set UID，简称为 SUID 的特殊权限。 那么SUID的权限对於一个文件的特殊功能是什么呢？基本上SUID有这样的限制与功能：</p><ul><li>SUID 权限仅对二进位程序(binary program)有效；</li><li>运行者对於该程序需要具有 x 的可运行权限；</li><li>本权限仅在运行该程序的过程中有效 (run-time)；</li><li>运行者将具有该程序拥有者 (owner) 的权限。</li></ul></blockquote><p>这样就解决了运行权限的问题，由于我们只封装了容器作业必要的基本指令，所以权限的提升不会带来严重的安全问题。</p><p>对于资源限制的问题，本质上Slurm的资源限制是通过<code>cgroup</code>实现的，所以我们只需要将容器内运行的进程定位到，再将其放到对应的<code>Slurm</code>作业所在的资源限制组中，就实现了资源限制，关于上述问题的详细实现方式可以直接读<a href="https://github.com/ansiz/go-socker" target="_blank" rel="noopener">go-socker</a>的代码了解，也欢迎提交issue和PR！</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://ieeexplore.ieee.org/document/7923813/" target="_blank" rel="noopener"><em>Enabling Docker Containers for High-Performance and Many-Task Computing</em></a></li><li><a href="http://dockone.io/article/589" target="_blank" rel="noopener"><em>为什么我们不允许非root用户在CentOS、Fedora和RHEL上直接运行Docker命令</em></a></li><li><a href="http://cn.linux.vbird.org/linux_basic/0220filemanager_4.php#suid" target="_blank" rel="noopener">鸟哥的Linux私房菜——第七章、Linux文件与目录管理</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://img.chinahpc.club/2018/07/docker-banner2.jpg&quot; alt=&quot;docker&quot;&gt;&lt;br&gt;HPC应用总类繁多，各种软件可能运行在不同的系统平台之上，容器技术正是解决这类问题的绝佳手段。我在&lt;a href=&quot;http://blog.zxh.site/2018/05/15/Singularity/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;em&gt;Singularity——HPC环境的绝佳容器解决方案&lt;/em&gt;&lt;/a&gt;一文中介绍了&lt;code&gt;Singularity&lt;/code&gt;这个HPC下非常适合的容器技术。但是Singularity也有一些不尽人意的地方，例如缺少虚拟化网络、热度不如&lt;code&gt;Docker&lt;/code&gt;高等。那如果想在一个高性能集群中使用Docker又会遇到什么问题呢？有没有什么好的解决方案？
    
    </summary>
    
      <category term="容器" scheme="http://zxh.site/categories/%E5%AE%B9%E5%99%A8/"/>
    
      <category term="HPC" scheme="http://zxh.site/categories/%E5%AE%B9%E5%99%A8/HPC/"/>
    
    
      <category term="HPC" scheme="http://zxh.site/tags/HPC/"/>
    
      <category term="Docker" scheme="http://zxh.site/tags/Docker/"/>
    
      <category term="Slurm" scheme="http://zxh.site/tags/Slurm/"/>
    
  </entry>
  
  <entry>
    <title>有趣的X11</title>
    <link href="http://zxh.site/2018/06/24/x11/"/>
    <id>http://zxh.site/2018/06/24/x11/</id>
    <published>2018-06-24T08:07:04.000Z</published>
    <updated>2018-10-21T10:57:45.883Z</updated>
    
    <content type="html"><![CDATA[<p>大多数熟练使用Linux的用户都习惯于使用高效的命令行进行操作，尤其是在服务器上，安装桌面环境浪费资源、降低稳定性。但是有的应用程序又必须有显示输出才有意义，比如使用服务器运行一些有图像输出的程序时。你可以选择安装桌面环境，然后使用<code>VNC</code>或<code>RDP</code>进行远程桌面连接，但更好的选择其实是使用X窗口系统（以下简称X11）转发输出。这张图片就是一个神奇的Linux文件管理器和macOS文件管理器”运行”在同一桌面下的有趣效果：<br><img src="http://img.chinahpc.club/2018/05/x11-fs8.png" alt="x11"><br><a id="more"></a></p><h2 id="什么是X11？"><a href="#什么是X11？" class="headerlink" title="什么是X11？"></a>什么是X11？</h2><p>这项有趣的技术自1984年第一版X1发布至今已经有三十多年，可以称得上是古老的技术了。1987年9月15日首次发布现有X窗口系统所用的协议<code>X11</code>（Protocol），后续发布的版本命名为<code>X11R2</code>、<code>X11R3</code>…而今最新的引用实现（引用性、示范性的实现体）版本则是2012年发布的X11R7.7（X11 Release 7.7），所以后来就简称<code>X11</code>了。</p><blockquote><p>X窗口系统（X Window System，也常称为X11或X）是一种以位图方式显示的软件窗口系统。最初是1984年麻省理工学院的研究，之后变成UNIX、类UNIX、以及OpenVMS等操作系统所一致适用的标准化软件工具包及显示架构的运作协定。X窗口系统通过软件工具及架构协定来创建操作系统所用的图形用户界面，此后则逐渐扩展适用到各形各色的其他操作系统上。现在几乎所有的操作系统都能支持与使用X。更重要的是，今日知名的桌面环境——GNOME和KDE也都是以X窗口系统为基础建构成的。————<a href="https://zh.wikipedia.org/wiki/X_Window%E7%B3%BB%E7%B5%B1#.E5.8F.82.E8.A7.81" target="_blank" rel="noopener">维基百科-X窗口系统</a></p></blockquote><p><code>X11</code>最大的特点是它的C/S架构和我们一般认为的Server运行在远端Client运行在本地恰好相反，<code>X11</code>的Server反而通常运行在用户本地的机器上，而运行在远端的Client也并非特指某一个或一种程序，而是一切基于<code>X11</code>开发的GUI。通常我们把X Server安装在本地并启动，例如<code>Windows</code>上的<code>XManager</code>，<code>macOS</code>上的<code>XQuartz</code>等软件，用以接收远端的图像信号并将鼠标、键盘输入转发到远端。你可以把X11理解为将图像数据处理运行在服务器上，但显示信号通过网络输出到另外一台机器上，想象将平时我们使用显示器的HDMI或者DVI线被换成了一根网线。</p><h2 id="X11的优点"><a href="#X11的优点" class="headerlink" title="X11的优点"></a>X11的优点</h2><h3 id="让显示输出更加灵活"><a href="#让显示输出更加灵活" class="headerlink" title="让显示输出更加灵活"></a>让显示输出更加灵活</h3><p>客户端可以在远程服务器上执行计算任务，而X Server仅负责图形显示。既可以充分利用服务器的强大性能进行运算，还不用给服务器安装桌面环境或连接显示器就能看到图像输出，这种场景在集群环境下非常常见。</p><h3 id="让GUI变得可移植"><a href="#让GUI变得可移植" class="headerlink" title="让GUI变得可移植"></a>让GUI变得可移植</h3><p>只有X Server服务端与硬件打交道，所有的客户端都与硬件无关，这让不同的平台上的移植变得很容易。</p><h3 id="风格可自定义"><a href="#风格可自定义" class="headerlink" title="风格可自定义"></a>风格可自定义</h3><p>X系统只负责显示图形，并不限制显示和操作的风格，因此不同的X Window的风格并不相同，用户可以根据自己的喜好进行选择。</p><h2 id="X11的缺点"><a href="#X11的缺点" class="headerlink" title="X11的缺点"></a>X11的缺点</h2><h3 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h3><p>X Window的C/S体系（C/S架构）设计在应用程序和视频硬件之间多加了一层软件，导致绘图效率下降，所以引起了一些批评。因此开发了若干扩展，在设备和客户机在同一个系统上时，通过在获取适当许可的情况下，以直接访问设备来改善这一问题。而在Linux上，一些视频驱动已经部分移入内核以提高效率。</p><h3 id="稳定性"><a href="#稳定性" class="headerlink" title="稳定性"></a>稳定性</h3><p>另一方面，X也被批评为需要（或者提供）了过多的对硬件的直接访问，从而影响了系统稳定性。行为不良的显卡驱动（有时也可能是应用程序）甚至能够导致整个系统崩溃或者重启；有时即使操作系统仍在工作，它也不能继续渲染其显示（这时除了重启，缺乏好的恢复手段）。目前所有的桌面GUI操作系统都提供某种对硬件的直接访问，支持者认为市场已经证明为了提供图形性能牺牲一点稳定性是值得的。或许将来随着技术和用户的演化这一平衡会有所变化。</p><h3 id="不规范的用户界面"><a href="#不规范的用户界面" class="headerlink" title="不规范的用户界面"></a>不规范的用户界面</h3><p>X刻意不去规范用户界面和程序之间大多数的通信，导致出现了许多非常不同的界面，同时造成程序之间协同的困难；而客户机之间的互操作规范ICCCM以难以正确实现而闻名。后来的标准化尝试，如Motif和CDE，也于事无补。长久以来这已经成为用户和程序员的噩梦。</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>X缺乏良好的网络与透明的音效系统。当用户对音效日益期盼时，各种不兼容的音效子系统便出现了。过去，大多数程序员只好忽略网络问题，简单地使用本地操作专用的音效API。第一代客户端-服务端音效系统有rplay和Network Audio System。而最近的努力产生了EsounD（GNOME）和ARts（KDE），而这也并非标准。而其他系统如Media Application Server则正在开发当中。<br>直到最近，X也没有好地解决显示与打印机所打印的内容一致性（所见即所得）的解决方案。许多X客户机完全用PostScript实现打印，而这与发送到服务端的几乎是分离的。从X11R6.3起包含了Xprint，此时客户端已经不错，但是服务器实现还不行。而从X11R6.8起实现的质量已经很好，并且获得了组件支持。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>X11是一个功能非常强大的 C/S 图形显示系统，具有很好的跨网络性能，也易于进行扩展，是一个值得学习研究的技术。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://zh.wikipedia.org/wiki/X_Window%E7%B3%BB%E7%B5%B1" target="_blank" rel="noopener">X窗口系统</a></li><li><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-xwin/" target="_blank" rel="noopener">X Window 系统的窗口显示原理</a></li><li><a href="https://baike.baidu.com/item/X%20Window/7249336" target="_blank" rel="noopener">X Window</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大多数熟练使用Linux的用户都习惯于使用高效的命令行进行操作，尤其是在服务器上，安装桌面环境浪费资源、降低稳定性。但是有的应用程序又必须有显示输出才有意义，比如使用服务器运行一些有图像输出的程序时。你可以选择安装桌面环境，然后使用&lt;code&gt;VNC&lt;/code&gt;或&lt;code&gt;RDP&lt;/code&gt;进行远程桌面连接，但更好的选择其实是使用X窗口系统（以下简称X11）转发输出。这张图片就是一个神奇的Linux文件管理器和macOS文件管理器”运行”在同一桌面下的有趣效果：&lt;br&gt;&lt;img src=&quot;http://img.chinahpc.club/2018/05/x11-fs8.png&quot; alt=&quot;x11&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://zxh.site/categories/Linux/"/>
    
      <category term="工具" scheme="http://zxh.site/categories/Linux/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Linux" scheme="http://zxh.site/tags/Linux/"/>
    
      <category term="工具" scheme="http://zxh.site/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
</feed>
